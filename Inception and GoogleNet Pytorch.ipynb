{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test Computer Vision #2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-xeau9zfZsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tar -xf cifar-10-python.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNU6nk8MTk52",
        "colab_type": "text"
      },
      "source": [
        "# Inception V1 ( With dimension reductions )\n",
        "Adding 1x1 conv stage "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeBFL1d8TkHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
        "        super(Inception, self).__init__()\n",
        "        # 1x1 conv branch\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(n1x1),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # 1x1 conv -> 3x3 conv branch\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n3x3red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n3x3),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # 1x1 conv -> 5x5 conv branch\n",
        "        self.b3 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n5x5red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # 3x3 pool -> 1x1 conv branch\n",
        "        self.b4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
        "            nn.BatchNorm2d(pool_planes),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.linear = nn.Linear(160000, 10)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.b1(x)\n",
        "        y2 = self.b2(x)\n",
        "        y3 = self.b3(x)\n",
        "        y4 = self.b4(x)\n",
        "        out = torch.cat([y1,y2,y3,y4], 1)\n",
        "        #print(out.size())\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RqI51axVXD8",
        "colab_type": "code",
        "outputId": "291ab69f-33bc-4e73-8115-494673fdc6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Was testing the model sizes\n",
        "def test():\n",
        "    net = Inception(3,  64,  96, 128, 16, 32, 32)\n",
        "    x = torch.randn(1,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "    print(y)\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 32, 32])\n",
            "torch.Size([1, 10])\n",
            "tensor([[ 0.2319, -0.3461, -0.3062, -0.1209,  0.1695, -0.0083,  0.1265, -0.0462,\n",
            "         -0.0766,  0.0283]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4dACvJzkEDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randn(1,3,32,32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we6jR1xFhGIi",
        "colab_type": "text"
      },
      "source": [
        "# GoogleNet (Inception V1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsTjxhAPdcYV",
        "colab_type": "code",
        "outputId": "f508a213-bda1-4720-cf42-5193ace5c516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''GoogLeNet with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
        "        super(Inception, self).__init__()\n",
        "        # 1x1 conv branch\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(n1x1),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # 1x1 conv -> 3x3 conv branch\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n3x3red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n3x3),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        #print(n5x5)\n",
        "        # 1x1 conv -> 5x5 conv branch\n",
        "        self.b3 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n5x5, kernel_size=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5, n5x5, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # 3x3 pool -> 1x1 conv branch\n",
        "        self.b4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
        "            nn.BatchNorm2d(pool_planes),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y1 = self.b1(x)\n",
        "        y2 = self.b2(x)\n",
        "        y3 = self.b3(x)\n",
        "        y4 = self.b4(x)\n",
        "        '''print(y1.size())\n",
        "        print(y2.size())\n",
        "        print(y3.size())\n",
        "        print(y4.size())'''\n",
        "        return torch.cat([y1,y2,y3,y4], 1)\n",
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        self.pre_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
        "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
        "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
        "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
        "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
        "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
        "\n",
        "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.linear = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pre_layers(x)\n",
        "        out = self.a3(out)\n",
        "        out = self.b3(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.a4(out)\n",
        "        out = self.b4(out)\n",
        "        out = self.c4(out)\n",
        "        out = self.d4(out)\n",
        "        out = self.e4(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.a5(out)\n",
        "        out = self.b5(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "'''def test():\n",
        "    net = GoogLeNet()\n",
        "    x = torch.randn(1,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "    print(y)\n",
        "\n",
        "test()'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def test():\\n    net = GoogLeNet()\\n    x = torch.randn(1,3,32,32)\\n    y = net(x)\\n    print(y.size())\\n    print(y)\\n\\ntest()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw-YnOeRndQb",
        "colab_type": "text"
      },
      "source": [
        "# Preparing DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYwHswvrnf7L",
        "colab_type": "code",
        "outputId": "5ab77ad0-c181-492a-fdd9-64962398856d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "'''\n",
        "transforms.Normalize does the following for each channel:\n",
        "image = (image - mean) / std\n",
        "Normalization does helps CNN perform better.\n",
        "Normalization helps get data within a range and reduces the skewness which helps learn faster and better\n",
        "'''\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYXSlx1polrc",
        "colab_type": "code",
        "outputId": "c9aed278-a478-4754-c296-da98868f7044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "'''Parameters of DataLoader\n",
        "        dataset (Dataset): dataset from which to load the data.\n",
        "        batch_size (int, optional): how many samples per batch to load\n",
        "            (default: ``1``).\n",
        "        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
        "            at every epoch (default: ``False``).\n",
        "        num_workers (int, optional): how many subprocesses to use for data\n",
        "            loading. ``0`` means that the data will be loaded in the main process.\n",
        "            (default: ``0``)\n",
        "'''\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91hUFrXFlh1X",
        "colab_type": "text"
      },
      "source": [
        "# Training GoogleNet ON CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U3a3HKXlkoS",
        "colab_type": "code",
        "outputId": "bfed44ad-876e-4294-b99f-75075ce8ba87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "#from utils import progress_bar\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "net = GoogLeNet()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "#Loss and Optimization\n",
        "criterion = nn.CrossEntropyLoss() # It is useful when training a classification problem with `C` classes.\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train() # Sets the model in training mode\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #print(inputs.size())\n",
        "        #print(targets)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() # UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "#Testing \n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval() # Sets the model in evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Building model..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBvvaK0sqkqZ",
        "colab_type": "code",
        "outputId": "4a750a7f-8e19-4711-cdbb-2250bcedfca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+2):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 2.309 | Acc: 10.938% (14/128)\n",
            "1 391 Loss: 2.313 | Acc: 10.938% (28/256)\n",
            "2 391 Loss: 2.295 | Acc: 12.240% (47/384)\n",
            "3 391 Loss: 2.285 | Acc: 12.305% (63/512)\n",
            "4 391 Loss: 2.254 | Acc: 13.281% (85/640)\n",
            "5 391 Loss: 2.237 | Acc: 13.802% (106/768)\n",
            "6 391 Loss: 2.207 | Acc: 15.290% (137/896)\n",
            "7 391 Loss: 2.172 | Acc: 17.188% (176/1024)\n",
            "8 391 Loss: 2.151 | Acc: 18.056% (208/1152)\n",
            "9 391 Loss: 2.134 | Acc: 18.984% (243/1280)\n",
            "10 391 Loss: 2.149 | Acc: 18.963% (267/1408)\n",
            "11 391 Loss: 2.147 | Acc: 19.466% (299/1536)\n",
            "12 391 Loss: 2.148 | Acc: 19.832% (330/1664)\n",
            "13 391 Loss: 2.167 | Acc: 19.754% (354/1792)\n",
            "14 391 Loss: 2.170 | Acc: 19.896% (382/1920)\n",
            "15 391 Loss: 2.164 | Acc: 20.264% (415/2048)\n",
            "16 391 Loss: 2.154 | Acc: 20.634% (449/2176)\n",
            "17 391 Loss: 2.144 | Acc: 21.224% (489/2304)\n",
            "18 391 Loss: 2.145 | Acc: 21.423% (521/2432)\n",
            "19 391 Loss: 2.140 | Acc: 21.641% (554/2560)\n",
            "20 391 Loss: 2.134 | Acc: 21.912% (589/2688)\n",
            "21 391 Loss: 2.122 | Acc: 22.301% (628/2816)\n",
            "22 391 Loss: 2.123 | Acc: 22.520% (663/2944)\n",
            "23 391 Loss: 2.119 | Acc: 22.852% (702/3072)\n",
            "24 391 Loss: 2.114 | Acc: 23.000% (736/3200)\n",
            "25 391 Loss: 2.114 | Acc: 23.047% (767/3328)\n",
            "26 391 Loss: 2.108 | Acc: 22.917% (792/3456)\n",
            "27 391 Loss: 2.103 | Acc: 23.298% (835/3584)\n",
            "28 391 Loss: 2.097 | Acc: 23.545% (874/3712)\n",
            "29 391 Loss: 2.087 | Acc: 23.724% (911/3840)\n",
            "30 391 Loss: 2.092 | Acc: 23.891% (948/3968)\n",
            "31 391 Loss: 2.089 | Acc: 24.048% (985/4096)\n",
            "32 391 Loss: 2.087 | Acc: 24.171% (1021/4224)\n",
            "33 391 Loss: 2.083 | Acc: 24.517% (1067/4352)\n",
            "34 391 Loss: 2.076 | Acc: 24.754% (1109/4480)\n",
            "35 391 Loss: 2.068 | Acc: 25.022% (1153/4608)\n",
            "36 391 Loss: 2.064 | Acc: 25.021% (1185/4736)\n",
            "37 391 Loss: 2.058 | Acc: 25.267% (1229/4864)\n",
            "38 391 Loss: 2.055 | Acc: 25.341% (1265/4992)\n",
            "39 391 Loss: 2.051 | Acc: 25.566% (1309/5120)\n",
            "40 391 Loss: 2.045 | Acc: 25.743% (1351/5248)\n",
            "41 391 Loss: 2.039 | Acc: 25.893% (1392/5376)\n",
            "42 391 Loss: 2.031 | Acc: 26.072% (1435/5504)\n",
            "43 391 Loss: 2.026 | Acc: 26.065% (1468/5632)\n",
            "44 391 Loss: 2.020 | Acc: 26.354% (1518/5760)\n",
            "45 391 Loss: 2.016 | Acc: 26.579% (1565/5888)\n",
            "46 391 Loss: 2.019 | Acc: 26.679% (1605/6016)\n",
            "47 391 Loss: 2.014 | Acc: 26.855% (1650/6144)\n",
            "48 391 Loss: 2.010 | Acc: 26.929% (1689/6272)\n",
            "49 391 Loss: 2.007 | Acc: 27.125% (1736/6400)\n",
            "50 391 Loss: 2.004 | Acc: 27.344% (1785/6528)\n",
            "51 391 Loss: 1.996 | Acc: 27.614% (1838/6656)\n",
            "52 391 Loss: 1.991 | Acc: 27.653% (1876/6784)\n",
            "53 391 Loss: 1.985 | Acc: 27.850% (1925/6912)\n",
            "54 391 Loss: 1.980 | Acc: 28.111% (1979/7040)\n",
            "55 391 Loss: 1.972 | Acc: 28.376% (2034/7168)\n",
            "56 391 Loss: 1.967 | Acc: 28.454% (2076/7296)\n",
            "57 391 Loss: 1.960 | Acc: 28.650% (2127/7424)\n",
            "58 391 Loss: 1.953 | Acc: 28.959% (2187/7552)\n",
            "59 391 Loss: 1.946 | Acc: 29.258% (2247/7680)\n",
            "60 391 Loss: 1.943 | Acc: 29.457% (2300/7808)\n",
            "61 391 Loss: 1.940 | Acc: 29.536% (2344/7936)\n",
            "62 391 Loss: 1.937 | Acc: 29.588% (2386/8064)\n",
            "63 391 Loss: 1.934 | Acc: 29.626% (2427/8192)\n",
            "64 391 Loss: 1.932 | Acc: 29.700% (2471/8320)\n",
            "65 391 Loss: 1.927 | Acc: 29.901% (2526/8448)\n",
            "66 391 Loss: 1.922 | Acc: 30.084% (2580/8576)\n",
            "67 391 Loss: 1.918 | Acc: 30.239% (2632/8704)\n",
            "68 391 Loss: 1.913 | Acc: 30.423% (2687/8832)\n",
            "69 391 Loss: 1.911 | Acc: 30.513% (2734/8960)\n",
            "70 391 Loss: 1.907 | Acc: 30.656% (2786/9088)\n",
            "71 391 Loss: 1.901 | Acc: 30.838% (2842/9216)\n",
            "72 391 Loss: 1.895 | Acc: 30.993% (2896/9344)\n",
            "73 391 Loss: 1.893 | Acc: 31.092% (2945/9472)\n",
            "74 391 Loss: 1.889 | Acc: 31.198% (2995/9600)\n",
            "75 391 Loss: 1.885 | Acc: 31.394% (3054/9728)\n",
            "76 391 Loss: 1.881 | Acc: 31.544% (3109/9856)\n",
            "77 391 Loss: 1.880 | Acc: 31.661% (3161/9984)\n",
            "78 391 Loss: 1.877 | Acc: 31.784% (3214/10112)\n",
            "79 391 Loss: 1.876 | Acc: 31.836% (3260/10240)\n",
            "80 391 Loss: 1.875 | Acc: 31.896% (3307/10368)\n",
            "81 391 Loss: 1.871 | Acc: 31.984% (3357/10496)\n",
            "82 391 Loss: 1.867 | Acc: 32.088% (3409/10624)\n",
            "83 391 Loss: 1.863 | Acc: 32.217% (3464/10752)\n",
            "84 391 Loss: 1.859 | Acc: 32.399% (3525/10880)\n",
            "85 391 Loss: 1.855 | Acc: 32.513% (3579/11008)\n",
            "86 391 Loss: 1.852 | Acc: 32.606% (3631/11136)\n",
            "87 391 Loss: 1.852 | Acc: 32.635% (3676/11264)\n",
            "88 391 Loss: 1.850 | Acc: 32.707% (3726/11392)\n",
            "89 391 Loss: 1.847 | Acc: 32.786% (3777/11520)\n",
            "90 391 Loss: 1.844 | Acc: 32.907% (3833/11648)\n",
            "91 391 Loss: 1.843 | Acc: 32.991% (3885/11776)\n",
            "92 391 Loss: 1.839 | Acc: 33.123% (3943/11904)\n",
            "93 391 Loss: 1.837 | Acc: 33.153% (3989/12032)\n",
            "94 391 Loss: 1.833 | Acc: 33.273% (4046/12160)\n",
            "95 391 Loss: 1.831 | Acc: 33.390% (4103/12288)\n",
            "96 391 Loss: 1.827 | Acc: 33.497% (4159/12416)\n",
            "97 391 Loss: 1.822 | Acc: 33.689% (4226/12544)\n",
            "98 391 Loss: 1.818 | Acc: 33.909% (4297/12672)\n",
            "99 391 Loss: 1.815 | Acc: 34.031% (4356/12800)\n",
            "100 391 Loss: 1.813 | Acc: 34.035% (4400/12928)\n",
            "101 391 Loss: 1.809 | Acc: 34.138% (4457/13056)\n",
            "102 391 Loss: 1.805 | Acc: 34.292% (4521/13184)\n",
            "103 391 Loss: 1.803 | Acc: 34.375% (4576/13312)\n",
            "104 391 Loss: 1.800 | Acc: 34.494% (4636/13440)\n",
            "105 391 Loss: 1.797 | Acc: 34.640% (4700/13568)\n",
            "106 391 Loss: 1.795 | Acc: 34.718% (4755/13696)\n",
            "107 391 Loss: 1.793 | Acc: 34.845% (4817/13824)\n",
            "108 391 Loss: 1.790 | Acc: 34.956% (4877/13952)\n",
            "109 391 Loss: 1.786 | Acc: 35.028% (4932/14080)\n",
            "110 391 Loss: 1.785 | Acc: 35.058% (4981/14208)\n",
            "111 391 Loss: 1.784 | Acc: 35.128% (5036/14336)\n",
            "112 391 Loss: 1.781 | Acc: 35.260% (5100/14464)\n",
            "113 391 Loss: 1.778 | Acc: 35.328% (5155/14592)\n",
            "114 391 Loss: 1.778 | Acc: 35.374% (5207/14720)\n",
            "115 391 Loss: 1.777 | Acc: 35.405% (5257/14848)\n",
            "116 391 Loss: 1.774 | Acc: 35.477% (5313/14976)\n",
            "117 391 Loss: 1.771 | Acc: 35.540% (5368/15104)\n",
            "118 391 Loss: 1.769 | Acc: 35.629% (5427/15232)\n",
            "119 391 Loss: 1.766 | Acc: 35.762% (5493/15360)\n",
            "120 391 Loss: 1.763 | Acc: 35.828% (5549/15488)\n",
            "121 391 Loss: 1.761 | Acc: 35.899% (5606/15616)\n",
            "122 391 Loss: 1.759 | Acc: 35.969% (5663/15744)\n",
            "123 391 Loss: 1.757 | Acc: 36.076% (5726/15872)\n",
            "124 391 Loss: 1.755 | Acc: 36.087% (5774/16000)\n",
            "125 391 Loss: 1.755 | Acc: 36.148% (5830/16128)\n",
            "126 391 Loss: 1.753 | Acc: 36.257% (5894/16256)\n",
            "127 391 Loss: 1.751 | Acc: 36.298% (5947/16384)\n",
            "128 391 Loss: 1.749 | Acc: 36.367% (6005/16512)\n",
            "129 391 Loss: 1.747 | Acc: 36.496% (6073/16640)\n",
            "130 391 Loss: 1.745 | Acc: 36.570% (6132/16768)\n",
            "131 391 Loss: 1.743 | Acc: 36.583% (6181/16896)\n",
            "132 391 Loss: 1.740 | Acc: 36.678% (6244/17024)\n",
            "133 391 Loss: 1.738 | Acc: 36.777% (6308/17152)\n",
            "134 391 Loss: 1.736 | Acc: 36.782% (6356/17280)\n",
            "135 391 Loss: 1.734 | Acc: 36.857% (6416/17408)\n",
            "136 391 Loss: 1.731 | Acc: 36.981% (6485/17536)\n",
            "137 391 Loss: 1.728 | Acc: 37.053% (6545/17664)\n",
            "138 391 Loss: 1.725 | Acc: 37.196% (6618/17792)\n",
            "139 391 Loss: 1.723 | Acc: 37.310% (6686/17920)\n",
            "140 391 Loss: 1.720 | Acc: 37.417% (6753/18048)\n",
            "141 391 Loss: 1.718 | Acc: 37.506% (6817/18176)\n",
            "142 391 Loss: 1.714 | Acc: 37.637% (6889/18304)\n",
            "143 391 Loss: 1.712 | Acc: 37.739% (6956/18432)\n",
            "144 391 Loss: 1.710 | Acc: 37.791% (7014/18560)\n",
            "145 391 Loss: 1.707 | Acc: 37.912% (7085/18688)\n",
            "146 391 Loss: 1.703 | Acc: 38.079% (7165/18816)\n",
            "147 391 Loss: 1.700 | Acc: 38.181% (7233/18944)\n",
            "148 391 Loss: 1.698 | Acc: 38.224% (7290/19072)\n",
            "149 391 Loss: 1.697 | Acc: 38.276% (7349/19200)\n",
            "150 391 Loss: 1.695 | Acc: 38.395% (7421/19328)\n",
            "151 391 Loss: 1.692 | Acc: 38.507% (7492/19456)\n",
            "152 391 Loss: 1.690 | Acc: 38.593% (7558/19584)\n",
            "153 391 Loss: 1.689 | Acc: 38.621% (7613/19712)\n",
            "154 391 Loss: 1.687 | Acc: 38.715% (7681/19840)\n",
            "155 391 Loss: 1.685 | Acc: 38.752% (7738/19968)\n",
            "156 391 Loss: 1.684 | Acc: 38.814% (7800/20096)\n",
            "157 391 Loss: 1.682 | Acc: 38.889% (7865/20224)\n",
            "158 391 Loss: 1.681 | Acc: 38.949% (7927/20352)\n",
            "159 391 Loss: 1.678 | Acc: 39.023% (7992/20480)\n",
            "160 391 Loss: 1.677 | Acc: 39.082% (8054/20608)\n",
            "161 391 Loss: 1.675 | Acc: 39.135% (8115/20736)\n",
            "162 391 Loss: 1.673 | Acc: 39.182% (8175/20864)\n",
            "163 391 Loss: 1.671 | Acc: 39.272% (8244/20992)\n",
            "164 391 Loss: 1.670 | Acc: 39.318% (8304/21120)\n",
            "165 391 Loss: 1.667 | Acc: 39.387% (8369/21248)\n",
            "166 391 Loss: 1.666 | Acc: 39.465% (8436/21376)\n",
            "167 391 Loss: 1.666 | Acc: 39.500% (8494/21504)\n",
            "168 391 Loss: 1.663 | Acc: 39.594% (8565/21632)\n",
            "169 391 Loss: 1.660 | Acc: 39.646% (8627/21760)\n",
            "170 391 Loss: 1.659 | Acc: 39.684% (8686/21888)\n",
            "171 391 Loss: 1.658 | Acc: 39.735% (8748/22016)\n",
            "172 391 Loss: 1.655 | Acc: 39.862% (8827/22144)\n",
            "173 391 Loss: 1.652 | Acc: 39.965% (8901/22272)\n",
            "174 391 Loss: 1.651 | Acc: 40.000% (8960/22400)\n",
            "175 391 Loss: 1.648 | Acc: 40.070% (9027/22528)\n",
            "176 391 Loss: 1.646 | Acc: 40.135% (9093/22656)\n",
            "177 391 Loss: 1.644 | Acc: 40.195% (9158/22784)\n",
            "178 391 Loss: 1.643 | Acc: 40.215% (9214/22912)\n",
            "179 391 Loss: 1.642 | Acc: 40.295% (9284/23040)\n",
            "180 391 Loss: 1.640 | Acc: 40.362% (9351/23168)\n",
            "181 391 Loss: 1.639 | Acc: 40.423% (9417/23296)\n",
            "182 391 Loss: 1.637 | Acc: 40.493% (9485/23424)\n",
            "183 391 Loss: 1.636 | Acc: 40.544% (9549/23552)\n",
            "184 391 Loss: 1.634 | Acc: 40.629% (9621/23680)\n",
            "185 391 Loss: 1.632 | Acc: 40.709% (9692/23808)\n",
            "186 391 Loss: 1.631 | Acc: 40.788% (9763/23936)\n",
            "187 391 Loss: 1.629 | Acc: 40.874% (9836/24064)\n",
            "188 391 Loss: 1.627 | Acc: 40.935% (9903/24192)\n",
            "189 391 Loss: 1.625 | Acc: 41.020% (9976/24320)\n",
            "190 391 Loss: 1.623 | Acc: 41.087% (10045/24448)\n",
            "191 391 Loss: 1.621 | Acc: 41.154% (10114/24576)\n",
            "192 391 Loss: 1.619 | Acc: 41.232% (10186/24704)\n",
            "193 391 Loss: 1.618 | Acc: 41.289% (10253/24832)\n",
            "194 391 Loss: 1.617 | Acc: 41.334% (10317/24960)\n",
            "195 391 Loss: 1.615 | Acc: 41.410% (10389/25088)\n",
            "196 391 Loss: 1.613 | Acc: 41.474% (10458/25216)\n",
            "197 391 Loss: 1.611 | Acc: 41.517% (10522/25344)\n",
            "198 391 Loss: 1.610 | Acc: 41.595% (10595/25472)\n",
            "199 391 Loss: 1.607 | Acc: 41.660% (10665/25600)\n",
            "200 391 Loss: 1.605 | Acc: 41.744% (10740/25728)\n",
            "201 391 Loss: 1.603 | Acc: 41.839% (10818/25856)\n",
            "202 391 Loss: 1.601 | Acc: 41.903% (10888/25984)\n",
            "203 391 Loss: 1.599 | Acc: 41.996% (10966/26112)\n",
            "204 391 Loss: 1.597 | Acc: 42.043% (11032/26240)\n",
            "205 391 Loss: 1.595 | Acc: 42.115% (11105/26368)\n",
            "206 391 Loss: 1.592 | Acc: 42.203% (11182/26496)\n",
            "207 391 Loss: 1.591 | Acc: 42.255% (11250/26624)\n",
            "208 391 Loss: 1.590 | Acc: 42.307% (11318/26752)\n",
            "209 391 Loss: 1.588 | Acc: 42.396% (11396/26880)\n",
            "210 391 Loss: 1.586 | Acc: 42.476% (11472/27008)\n",
            "211 391 Loss: 1.584 | Acc: 42.530% (11541/27136)\n",
            "212 391 Loss: 1.582 | Acc: 42.584% (11610/27264)\n",
            "213 391 Loss: 1.580 | Acc: 42.647% (11682/27392)\n",
            "214 391 Loss: 1.578 | Acc: 42.725% (11758/27520)\n",
            "215 391 Loss: 1.577 | Acc: 42.788% (11830/27648)\n",
            "216 391 Loss: 1.575 | Acc: 42.828% (11896/27776)\n",
            "217 391 Loss: 1.574 | Acc: 42.872% (11963/27904)\n",
            "218 391 Loss: 1.571 | Acc: 42.922% (12032/28032)\n",
            "219 391 Loss: 1.570 | Acc: 42.965% (12099/28160)\n",
            "220 391 Loss: 1.568 | Acc: 43.054% (12179/28288)\n",
            "221 391 Loss: 1.567 | Acc: 43.110% (12250/28416)\n",
            "222 391 Loss: 1.565 | Acc: 43.172% (12323/28544)\n",
            "223 391 Loss: 1.563 | Acc: 43.241% (12398/28672)\n",
            "224 391 Loss: 1.561 | Acc: 43.316% (12475/28800)\n",
            "225 391 Loss: 1.559 | Acc: 43.394% (12553/28928)\n",
            "226 391 Loss: 1.557 | Acc: 43.426% (12618/29056)\n",
            "227 391 Loss: 1.555 | Acc: 43.503% (12696/29184)\n",
            "228 391 Loss: 1.554 | Acc: 43.542% (12763/29312)\n",
            "229 391 Loss: 1.553 | Acc: 43.570% (12827/29440)\n",
            "230 391 Loss: 1.551 | Acc: 43.649% (12906/29568)\n",
            "231 391 Loss: 1.549 | Acc: 43.706% (12979/29696)\n",
            "232 391 Loss: 1.548 | Acc: 43.747% (13047/29824)\n",
            "233 391 Loss: 1.548 | Acc: 43.777% (13112/29952)\n",
            "234 391 Loss: 1.547 | Acc: 43.797% (13174/30080)\n",
            "235 391 Loss: 1.546 | Acc: 43.839% (13243/30208)\n",
            "236 391 Loss: 1.546 | Acc: 43.875% (13310/30336)\n",
            "237 391 Loss: 1.544 | Acc: 43.924% (13381/30464)\n",
            "238 391 Loss: 1.544 | Acc: 43.956% (13447/30592)\n",
            "239 391 Loss: 1.541 | Acc: 44.040% (13529/30720)\n",
            "240 391 Loss: 1.540 | Acc: 44.094% (13602/30848)\n",
            "241 391 Loss: 1.538 | Acc: 44.163% (13680/30976)\n",
            "242 391 Loss: 1.537 | Acc: 44.223% (13755/31104)\n",
            "243 391 Loss: 1.535 | Acc: 44.291% (13833/31232)\n",
            "244 391 Loss: 1.533 | Acc: 44.349% (13908/31360)\n",
            "245 391 Loss: 1.532 | Acc: 44.395% (13979/31488)\n",
            "246 391 Loss: 1.530 | Acc: 44.471% (14060/31616)\n",
            "247 391 Loss: 1.528 | Acc: 44.547% (14141/31744)\n",
            "248 391 Loss: 1.526 | Acc: 44.610% (14218/31872)\n",
            "249 391 Loss: 1.525 | Acc: 44.666% (14293/32000)\n",
            "250 391 Loss: 1.523 | Acc: 44.749% (14377/32128)\n",
            "251 391 Loss: 1.522 | Acc: 44.789% (14447/32256)\n",
            "252 391 Loss: 1.522 | Acc: 44.822% (14515/32384)\n",
            "253 391 Loss: 1.521 | Acc: 44.851% (14582/32512)\n",
            "254 391 Loss: 1.520 | Acc: 44.902% (14656/32640)\n",
            "255 391 Loss: 1.519 | Acc: 44.937% (14725/32768)\n",
            "256 391 Loss: 1.517 | Acc: 45.002% (14804/32896)\n",
            "257 391 Loss: 1.516 | Acc: 45.064% (14882/33024)\n",
            "258 391 Loss: 1.515 | Acc: 45.125% (14960/33152)\n",
            "259 391 Loss: 1.513 | Acc: 45.174% (15034/33280)\n",
            "260 391 Loss: 1.512 | Acc: 45.241% (15114/33408)\n",
            "261 391 Loss: 1.511 | Acc: 45.298% (15191/33536)\n",
            "262 391 Loss: 1.510 | Acc: 45.330% (15260/33664)\n",
            "263 391 Loss: 1.508 | Acc: 45.372% (15332/33792)\n",
            "264 391 Loss: 1.507 | Acc: 45.433% (15411/33920)\n",
            "265 391 Loss: 1.506 | Acc: 45.474% (15483/34048)\n",
            "266 391 Loss: 1.505 | Acc: 45.529% (15560/34176)\n",
            "267 391 Loss: 1.503 | Acc: 45.598% (15642/34304)\n",
            "268 391 Loss: 1.502 | Acc: 45.641% (15715/34432)\n",
            "269 391 Loss: 1.501 | Acc: 45.697% (15793/34560)\n",
            "270 391 Loss: 1.500 | Acc: 45.725% (15861/34688)\n",
            "271 391 Loss: 1.498 | Acc: 45.766% (15934/34816)\n",
            "272 391 Loss: 1.498 | Acc: 45.805% (16006/34944)\n",
            "273 391 Loss: 1.496 | Acc: 45.880% (16091/35072)\n",
            "274 391 Loss: 1.495 | Acc: 45.918% (16163/35200)\n",
            "275 391 Loss: 1.493 | Acc: 45.998% (16250/35328)\n",
            "276 391 Loss: 1.491 | Acc: 46.051% (16328/35456)\n",
            "277 391 Loss: 1.491 | Acc: 46.094% (16402/35584)\n",
            "278 391 Loss: 1.490 | Acc: 46.119% (16470/35712)\n",
            "279 391 Loss: 1.489 | Acc: 46.175% (16549/35840)\n",
            "280 391 Loss: 1.488 | Acc: 46.191% (16614/35968)\n",
            "281 391 Loss: 1.487 | Acc: 46.266% (16700/36096)\n",
            "282 391 Loss: 1.486 | Acc: 46.270% (16761/36224)\n",
            "283 391 Loss: 1.485 | Acc: 46.306% (16833/36352)\n",
            "284 391 Loss: 1.485 | Acc: 46.305% (16892/36480)\n",
            "285 391 Loss: 1.484 | Acc: 46.337% (16963/36608)\n",
            "286 391 Loss: 1.483 | Acc: 46.388% (17041/36736)\n",
            "287 391 Loss: 1.481 | Acc: 46.441% (17120/36864)\n",
            "288 391 Loss: 1.481 | Acc: 46.472% (17191/36992)\n",
            "289 391 Loss: 1.479 | Acc: 46.509% (17264/37120)\n",
            "290 391 Loss: 1.479 | Acc: 46.518% (17327/37248)\n",
            "291 391 Loss: 1.478 | Acc: 46.562% (17403/37376)\n",
            "292 391 Loss: 1.477 | Acc: 46.598% (17476/37504)\n",
            "293 391 Loss: 1.476 | Acc: 46.631% (17548/37632)\n",
            "294 391 Loss: 1.475 | Acc: 46.660% (17619/37760)\n",
            "295 391 Loss: 1.474 | Acc: 46.698% (17693/37888)\n",
            "296 391 Loss: 1.473 | Acc: 46.730% (17765/38016)\n",
            "297 391 Loss: 1.472 | Acc: 46.770% (17840/38144)\n",
            "298 391 Loss: 1.471 | Acc: 46.791% (17908/38272)\n",
            "299 391 Loss: 1.470 | Acc: 46.836% (17985/38400)\n",
            "300 391 Loss: 1.469 | Acc: 46.875% (18060/38528)\n",
            "301 391 Loss: 1.467 | Acc: 46.932% (18142/38656)\n",
            "302 391 Loss: 1.466 | Acc: 46.968% (18216/38784)\n",
            "303 391 Loss: 1.465 | Acc: 47.016% (18295/38912)\n",
            "304 391 Loss: 1.464 | Acc: 47.047% (18367/39040)\n",
            "305 391 Loss: 1.462 | Acc: 47.089% (18444/39168)\n",
            "306 391 Loss: 1.461 | Acc: 47.117% (18515/39296)\n",
            "307 391 Loss: 1.460 | Acc: 47.169% (18596/39424)\n",
            "308 391 Loss: 1.458 | Acc: 47.229% (18680/39552)\n",
            "309 391 Loss: 1.457 | Acc: 47.276% (18759/39680)\n",
            "310 391 Loss: 1.456 | Acc: 47.322% (18838/39808)\n",
            "311 391 Loss: 1.455 | Acc: 47.356% (18912/39936)\n",
            "312 391 Loss: 1.453 | Acc: 47.399% (18990/40064)\n",
            "313 391 Loss: 1.452 | Acc: 47.445% (19069/40192)\n",
            "314 391 Loss: 1.451 | Acc: 47.475% (19142/40320)\n",
            "315 391 Loss: 1.449 | Acc: 47.523% (19222/40448)\n",
            "316 391 Loss: 1.447 | Acc: 47.587% (19309/40576)\n",
            "317 391 Loss: 1.446 | Acc: 47.612% (19380/40704)\n",
            "318 391 Loss: 1.445 | Acc: 47.671% (19465/40832)\n",
            "319 391 Loss: 1.444 | Acc: 47.727% (19549/40960)\n",
            "320 391 Loss: 1.442 | Acc: 47.788% (19635/41088)\n",
            "321 391 Loss: 1.441 | Acc: 47.843% (19719/41216)\n",
            "322 391 Loss: 1.440 | Acc: 47.888% (19799/41344)\n",
            "323 391 Loss: 1.438 | Acc: 47.938% (19881/41472)\n",
            "324 391 Loss: 1.437 | Acc: 47.986% (19962/41600)\n",
            "325 391 Loss: 1.436 | Acc: 48.042% (20047/41728)\n",
            "326 391 Loss: 1.434 | Acc: 48.084% (20126/41856)\n",
            "327 391 Loss: 1.433 | Acc: 48.123% (20204/41984)\n",
            "328 391 Loss: 1.433 | Acc: 48.157% (20280/42112)\n",
            "329 391 Loss: 1.431 | Acc: 48.203% (20361/42240)\n",
            "330 391 Loss: 1.430 | Acc: 48.239% (20438/42368)\n",
            "331 391 Loss: 1.429 | Acc: 48.287% (20520/42496)\n",
            "332 391 Loss: 1.427 | Acc: 48.351% (20609/42624)\n",
            "333 391 Loss: 1.425 | Acc: 48.428% (20704/42752)\n",
            "334 391 Loss: 1.424 | Acc: 48.479% (20788/42880)\n",
            "335 391 Loss: 1.423 | Acc: 48.514% (20865/43008)\n",
            "336 391 Loss: 1.422 | Acc: 48.556% (20945/43136)\n",
            "337 391 Loss: 1.420 | Acc: 48.618% (21034/43264)\n",
            "338 391 Loss: 1.420 | Acc: 48.645% (21108/43392)\n",
            "339 391 Loss: 1.418 | Acc: 48.697% (21193/43520)\n",
            "340 391 Loss: 1.417 | Acc: 48.749% (21278/43648)\n",
            "341 391 Loss: 1.415 | Acc: 48.794% (21360/43776)\n",
            "342 391 Loss: 1.414 | Acc: 48.822% (21435/43904)\n",
            "343 391 Loss: 1.413 | Acc: 48.871% (21519/44032)\n",
            "344 391 Loss: 1.412 | Acc: 48.927% (21606/44160)\n",
            "345 391 Loss: 1.410 | Acc: 48.961% (21684/44288)\n",
            "346 391 Loss: 1.409 | Acc: 49.003% (21765/44416)\n",
            "347 391 Loss: 1.408 | Acc: 49.030% (21840/44544)\n",
            "348 391 Loss: 1.407 | Acc: 49.071% (21921/44672)\n",
            "349 391 Loss: 1.406 | Acc: 49.116% (22004/44800)\n",
            "350 391 Loss: 1.405 | Acc: 49.145% (22080/44928)\n",
            "351 391 Loss: 1.404 | Acc: 49.181% (22159/45056)\n",
            "352 391 Loss: 1.403 | Acc: 49.203% (22232/45184)\n",
            "353 391 Loss: 1.402 | Acc: 49.230% (22307/45312)\n",
            "354 391 Loss: 1.400 | Acc: 49.280% (22393/45440)\n",
            "355 391 Loss: 1.399 | Acc: 49.320% (22474/45568)\n",
            "356 391 Loss: 1.398 | Acc: 49.376% (22563/45696)\n",
            "357 391 Loss: 1.397 | Acc: 49.406% (22640/45824)\n",
            "358 391 Loss: 1.396 | Acc: 49.452% (22724/45952)\n",
            "359 391 Loss: 1.395 | Acc: 49.486% (22803/46080)\n",
            "360 391 Loss: 1.394 | Acc: 49.537% (22890/46208)\n",
            "361 391 Loss: 1.393 | Acc: 49.573% (22970/46336)\n",
            "362 391 Loss: 1.392 | Acc: 49.591% (23042/46464)\n",
            "363 391 Loss: 1.391 | Acc: 49.609% (23114/46592)\n",
            "364 391 Loss: 1.391 | Acc: 49.623% (23184/46720)\n",
            "365 391 Loss: 1.389 | Acc: 49.665% (23267/46848)\n",
            "366 391 Loss: 1.389 | Acc: 49.702% (23348/46976)\n",
            "367 391 Loss: 1.389 | Acc: 49.722% (23421/47104)\n",
            "368 391 Loss: 1.388 | Acc: 49.733% (23490/47232)\n",
            "369 391 Loss: 1.387 | Acc: 49.768% (23570/47360)\n",
            "370 391 Loss: 1.387 | Acc: 49.789% (23644/47488)\n",
            "371 391 Loss: 1.386 | Acc: 49.824% (23724/47616)\n",
            "372 391 Loss: 1.384 | Acc: 49.872% (23811/47744)\n",
            "373 391 Loss: 1.383 | Acc: 49.912% (23894/47872)\n",
            "374 391 Loss: 1.383 | Acc: 49.946% (23974/48000)\n",
            "375 391 Loss: 1.381 | Acc: 50.002% (24065/48128)\n",
            "376 391 Loss: 1.380 | Acc: 50.044% (24149/48256)\n",
            "377 391 Loss: 1.379 | Acc: 50.074% (24228/48384)\n",
            "378 391 Loss: 1.379 | Acc: 50.087% (24298/48512)\n",
            "379 391 Loss: 1.378 | Acc: 50.111% (24374/48640)\n",
            "380 391 Loss: 1.377 | Acc: 50.156% (24460/48768)\n",
            "381 391 Loss: 1.376 | Acc: 50.200% (24546/48896)\n",
            "382 391 Loss: 1.374 | Acc: 50.253% (24636/49024)\n",
            "383 391 Loss: 1.373 | Acc: 50.311% (24729/49152)\n",
            "384 391 Loss: 1.372 | Acc: 50.351% (24813/49280)\n",
            "385 391 Loss: 1.371 | Acc: 50.403% (24903/49408)\n",
            "386 391 Loss: 1.370 | Acc: 50.442% (24987/49536)\n",
            "387 391 Loss: 1.369 | Acc: 50.471% (25066/49664)\n",
            "388 391 Loss: 1.368 | Acc: 50.522% (25156/49792)\n",
            "389 391 Loss: 1.367 | Acc: 50.567% (25243/49920)\n",
            "390 391 Loss: 1.366 | Acc: 50.600% (25300/50000)\n",
            "0 100 Loss: 1.343 | Acc: 52.000% (52/100)\n",
            "1 100 Loss: 1.248 | Acc: 55.000% (110/200)\n",
            "2 100 Loss: 1.242 | Acc: 55.333% (166/300)\n",
            "3 100 Loss: 1.246 | Acc: 54.250% (217/400)\n",
            "4 100 Loss: 1.239 | Acc: 55.000% (275/500)\n",
            "5 100 Loss: 1.223 | Acc: 55.333% (332/600)\n",
            "6 100 Loss: 1.244 | Acc: 55.143% (386/700)\n",
            "7 100 Loss: 1.252 | Acc: 55.125% (441/800)\n",
            "8 100 Loss: 1.236 | Acc: 55.778% (502/900)\n",
            "9 100 Loss: 1.245 | Acc: 55.700% (557/1000)\n",
            "10 100 Loss: 1.258 | Acc: 55.364% (609/1100)\n",
            "11 100 Loss: 1.259 | Acc: 55.250% (663/1200)\n",
            "12 100 Loss: 1.246 | Acc: 55.615% (723/1300)\n",
            "13 100 Loss: 1.225 | Acc: 56.357% (789/1400)\n",
            "14 100 Loss: 1.223 | Acc: 56.933% (854/1500)\n",
            "15 100 Loss: 1.224 | Acc: 56.750% (908/1600)\n",
            "16 100 Loss: 1.225 | Acc: 57.000% (969/1700)\n",
            "17 100 Loss: 1.213 | Acc: 57.111% (1028/1800)\n",
            "18 100 Loss: 1.211 | Acc: 57.526% (1093/1900)\n",
            "19 100 Loss: 1.221 | Acc: 57.350% (1147/2000)\n",
            "20 100 Loss: 1.215 | Acc: 57.619% (1210/2100)\n",
            "21 100 Loss: 1.210 | Acc: 57.909% (1274/2200)\n",
            "22 100 Loss: 1.212 | Acc: 57.957% (1333/2300)\n",
            "23 100 Loss: 1.216 | Acc: 57.875% (1389/2400)\n",
            "24 100 Loss: 1.216 | Acc: 57.760% (1444/2500)\n",
            "25 100 Loss: 1.228 | Acc: 57.692% (1500/2600)\n",
            "26 100 Loss: 1.226 | Acc: 57.630% (1556/2700)\n",
            "27 100 Loss: 1.221 | Acc: 57.679% (1615/2800)\n",
            "28 100 Loss: 1.226 | Acc: 57.483% (1667/2900)\n",
            "29 100 Loss: 1.215 | Acc: 57.867% (1736/3000)\n",
            "30 100 Loss: 1.209 | Acc: 58.065% (1800/3100)\n",
            "31 100 Loss: 1.206 | Acc: 58.125% (1860/3200)\n",
            "32 100 Loss: 1.203 | Acc: 58.273% (1923/3300)\n",
            "33 100 Loss: 1.203 | Acc: 58.206% (1979/3400)\n",
            "34 100 Loss: 1.208 | Acc: 58.229% (2038/3500)\n",
            "35 100 Loss: 1.208 | Acc: 58.278% (2098/3600)\n",
            "36 100 Loss: 1.215 | Acc: 58.135% (2151/3700)\n",
            "37 100 Loss: 1.216 | Acc: 58.079% (2207/3800)\n",
            "38 100 Loss: 1.208 | Acc: 58.385% (2277/3900)\n",
            "39 100 Loss: 1.208 | Acc: 58.450% (2338/4000)\n",
            "40 100 Loss: 1.210 | Acc: 58.634% (2404/4100)\n",
            "41 100 Loss: 1.213 | Acc: 58.524% (2458/4200)\n",
            "42 100 Loss: 1.210 | Acc: 58.698% (2524/4300)\n",
            "43 100 Loss: 1.208 | Acc: 58.818% (2588/4400)\n",
            "44 100 Loss: 1.206 | Acc: 58.911% (2651/4500)\n",
            "45 100 Loss: 1.206 | Acc: 58.870% (2708/4600)\n",
            "46 100 Loss: 1.207 | Acc: 58.809% (2764/4700)\n",
            "47 100 Loss: 1.212 | Acc: 58.708% (2818/4800)\n",
            "48 100 Loss: 1.207 | Acc: 58.816% (2882/4900)\n",
            "49 100 Loss: 1.202 | Acc: 59.000% (2950/5000)\n",
            "50 100 Loss: 1.200 | Acc: 59.059% (3012/5100)\n",
            "51 100 Loss: 1.203 | Acc: 58.962% (3066/5200)\n",
            "52 100 Loss: 1.202 | Acc: 58.943% (3124/5300)\n",
            "53 100 Loss: 1.201 | Acc: 58.944% (3183/5400)\n",
            "54 100 Loss: 1.205 | Acc: 58.891% (3239/5500)\n",
            "55 100 Loss: 1.209 | Acc: 58.750% (3290/5600)\n",
            "56 100 Loss: 1.210 | Acc: 58.754% (3349/5700)\n",
            "57 100 Loss: 1.209 | Acc: 58.741% (3407/5800)\n",
            "58 100 Loss: 1.217 | Acc: 58.559% (3455/5900)\n",
            "59 100 Loss: 1.218 | Acc: 58.483% (3509/6000)\n",
            "60 100 Loss: 1.219 | Acc: 58.361% (3560/6100)\n",
            "61 100 Loss: 1.217 | Acc: 58.355% (3618/6200)\n",
            "62 100 Loss: 1.217 | Acc: 58.349% (3676/6300)\n",
            "63 100 Loss: 1.214 | Acc: 58.453% (3741/6400)\n",
            "64 100 Loss: 1.217 | Acc: 58.431% (3798/6500)\n",
            "65 100 Loss: 1.219 | Acc: 58.364% (3852/6600)\n",
            "66 100 Loss: 1.217 | Acc: 58.433% (3915/6700)\n",
            "67 100 Loss: 1.218 | Acc: 58.324% (3966/6800)\n",
            "68 100 Loss: 1.220 | Acc: 58.203% (4016/6900)\n",
            "69 100 Loss: 1.222 | Acc: 58.171% (4072/7000)\n",
            "70 100 Loss: 1.222 | Acc: 58.099% (4125/7100)\n",
            "71 100 Loss: 1.220 | Acc: 58.208% (4191/7200)\n",
            "72 100 Loss: 1.218 | Acc: 58.247% (4252/7300)\n",
            "73 100 Loss: 1.218 | Acc: 58.284% (4313/7400)\n",
            "74 100 Loss: 1.216 | Acc: 58.347% (4376/7500)\n",
            "75 100 Loss: 1.218 | Acc: 58.368% (4436/7600)\n",
            "76 100 Loss: 1.219 | Acc: 58.312% (4490/7700)\n",
            "77 100 Loss: 1.218 | Acc: 58.321% (4549/7800)\n",
            "78 100 Loss: 1.218 | Acc: 58.316% (4607/7900)\n",
            "79 100 Loss: 1.218 | Acc: 58.325% (4666/8000)\n",
            "80 100 Loss: 1.218 | Acc: 58.358% (4727/8100)\n",
            "81 100 Loss: 1.219 | Acc: 58.317% (4782/8200)\n",
            "82 100 Loss: 1.219 | Acc: 58.373% (4845/8300)\n",
            "83 100 Loss: 1.221 | Acc: 58.369% (4903/8400)\n",
            "84 100 Loss: 1.221 | Acc: 58.341% (4959/8500)\n",
            "85 100 Loss: 1.223 | Acc: 58.326% (5016/8600)\n",
            "86 100 Loss: 1.225 | Acc: 58.276% (5070/8700)\n",
            "87 100 Loss: 1.227 | Acc: 58.227% (5124/8800)\n",
            "88 100 Loss: 1.229 | Acc: 58.213% (5181/8900)\n",
            "89 100 Loss: 1.232 | Acc: 58.167% (5235/9000)\n",
            "90 100 Loss: 1.231 | Acc: 58.165% (5293/9100)\n",
            "91 100 Loss: 1.228 | Acc: 58.196% (5354/9200)\n",
            "92 100 Loss: 1.228 | Acc: 58.194% (5412/9300)\n",
            "93 100 Loss: 1.227 | Acc: 58.202% (5471/9400)\n",
            "94 100 Loss: 1.227 | Acc: 58.168% (5526/9500)\n",
            "95 100 Loss: 1.227 | Acc: 58.167% (5584/9600)\n",
            "96 100 Loss: 1.226 | Acc: 58.268% (5652/9700)\n",
            "97 100 Loss: 1.226 | Acc: 58.245% (5708/9800)\n",
            "98 100 Loss: 1.225 | Acc: 58.253% (5767/9900)\n",
            "99 100 Loss: 1.223 | Acc: 58.250% (5825/10000)\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 0.931 | Acc: 64.844% (83/128)\n",
            "1 391 Loss: 0.942 | Acc: 64.844% (166/256)\n",
            "2 391 Loss: 0.950 | Acc: 64.323% (247/384)\n",
            "3 391 Loss: 0.932 | Acc: 64.258% (329/512)\n",
            "4 391 Loss: 0.935 | Acc: 65.000% (416/640)\n",
            "5 391 Loss: 0.959 | Acc: 64.844% (498/768)\n",
            "6 391 Loss: 0.938 | Acc: 65.513% (587/896)\n",
            "7 391 Loss: 0.954 | Acc: 65.234% (668/1024)\n",
            "8 391 Loss: 0.965 | Acc: 65.365% (753/1152)\n",
            "9 391 Loss: 0.979 | Acc: 65.156% (834/1280)\n",
            "10 391 Loss: 0.981 | Acc: 65.270% (919/1408)\n",
            "11 391 Loss: 0.984 | Acc: 64.974% (998/1536)\n",
            "12 391 Loss: 0.979 | Acc: 64.904% (1080/1664)\n",
            "13 391 Loss: 0.986 | Acc: 64.676% (1159/1792)\n",
            "14 391 Loss: 1.002 | Acc: 63.854% (1226/1920)\n",
            "15 391 Loss: 1.004 | Acc: 63.672% (1304/2048)\n",
            "16 391 Loss: 0.998 | Acc: 63.649% (1385/2176)\n",
            "17 391 Loss: 1.004 | Acc: 63.672% (1467/2304)\n",
            "18 391 Loss: 0.998 | Acc: 63.898% (1554/2432)\n",
            "19 391 Loss: 0.995 | Acc: 64.102% (1641/2560)\n",
            "20 391 Loss: 0.999 | Acc: 63.914% (1718/2688)\n",
            "21 391 Loss: 1.012 | Acc: 63.423% (1786/2816)\n",
            "22 391 Loss: 1.013 | Acc: 63.213% (1861/2944)\n",
            "23 391 Loss: 1.008 | Acc: 63.704% (1957/3072)\n",
            "24 391 Loss: 1.004 | Acc: 63.812% (2042/3200)\n",
            "25 391 Loss: 1.004 | Acc: 64.002% (2130/3328)\n",
            "26 391 Loss: 1.006 | Acc: 63.860% (2207/3456)\n",
            "27 391 Loss: 1.008 | Acc: 63.951% (2292/3584)\n",
            "28 391 Loss: 1.007 | Acc: 63.928% (2373/3712)\n",
            "29 391 Loss: 1.006 | Acc: 63.854% (2452/3840)\n",
            "30 391 Loss: 1.008 | Acc: 63.684% (2527/3968)\n",
            "31 391 Loss: 1.007 | Acc: 63.843% (2615/4096)\n",
            "32 391 Loss: 1.003 | Acc: 63.991% (2703/4224)\n",
            "33 391 Loss: 1.002 | Acc: 64.062% (2788/4352)\n",
            "34 391 Loss: 1.003 | Acc: 64.062% (2870/4480)\n",
            "35 391 Loss: 1.001 | Acc: 64.106% (2954/4608)\n",
            "36 391 Loss: 0.999 | Acc: 64.084% (3035/4736)\n",
            "37 391 Loss: 0.998 | Acc: 64.165% (3121/4864)\n",
            "38 391 Loss: 0.995 | Acc: 64.343% (3212/4992)\n",
            "39 391 Loss: 0.995 | Acc: 64.414% (3298/5120)\n",
            "40 391 Loss: 0.995 | Acc: 64.463% (3383/5248)\n",
            "41 391 Loss: 0.994 | Acc: 64.546% (3470/5376)\n",
            "42 391 Loss: 0.992 | Acc: 64.680% (3560/5504)\n",
            "43 391 Loss: 0.992 | Acc: 64.737% (3646/5632)\n",
            "44 391 Loss: 0.990 | Acc: 64.896% (3738/5760)\n",
            "45 391 Loss: 0.990 | Acc: 65.014% (3828/5888)\n",
            "46 391 Loss: 0.988 | Acc: 65.043% (3913/6016)\n",
            "47 391 Loss: 0.986 | Acc: 65.153% (4003/6144)\n",
            "48 391 Loss: 0.983 | Acc: 65.210% (4090/6272)\n",
            "49 391 Loss: 0.983 | Acc: 65.266% (4177/6400)\n",
            "50 391 Loss: 0.981 | Acc: 65.273% (4261/6528)\n",
            "51 391 Loss: 0.979 | Acc: 65.415% (4354/6656)\n",
            "52 391 Loss: 0.978 | Acc: 65.448% (4440/6784)\n",
            "53 391 Loss: 0.977 | Acc: 65.538% (4530/6912)\n",
            "54 391 Loss: 0.976 | Acc: 65.582% (4617/7040)\n",
            "55 391 Loss: 0.972 | Acc: 65.723% (4711/7168)\n",
            "56 391 Loss: 0.969 | Acc: 65.789% (4800/7296)\n",
            "57 391 Loss: 0.970 | Acc: 65.760% (4882/7424)\n",
            "58 391 Loss: 0.968 | Acc: 65.824% (4971/7552)\n",
            "59 391 Loss: 0.966 | Acc: 65.859% (5058/7680)\n",
            "60 391 Loss: 0.966 | Acc: 65.843% (5141/7808)\n",
            "61 391 Loss: 0.964 | Acc: 65.915% (5231/7936)\n",
            "62 391 Loss: 0.961 | Acc: 65.997% (5322/8064)\n",
            "63 391 Loss: 0.960 | Acc: 66.028% (5409/8192)\n",
            "64 391 Loss: 0.957 | Acc: 66.118% (5501/8320)\n",
            "65 391 Loss: 0.954 | Acc: 66.241% (5596/8448)\n",
            "66 391 Loss: 0.954 | Acc: 66.243% (5681/8576)\n",
            "67 391 Loss: 0.952 | Acc: 66.280% (5769/8704)\n",
            "68 391 Loss: 0.952 | Acc: 66.270% (5853/8832)\n",
            "69 391 Loss: 0.953 | Acc: 66.217% (5933/8960)\n",
            "70 391 Loss: 0.949 | Acc: 66.318% (6027/9088)\n",
            "71 391 Loss: 0.950 | Acc: 66.276% (6108/9216)\n",
            "72 391 Loss: 0.948 | Acc: 66.363% (6201/9344)\n",
            "73 391 Loss: 0.949 | Acc: 66.364% (6286/9472)\n",
            "74 391 Loss: 0.948 | Acc: 66.438% (6378/9600)\n",
            "75 391 Loss: 0.945 | Acc: 66.530% (6472/9728)\n",
            "76 391 Loss: 0.944 | Acc: 66.538% (6558/9856)\n",
            "77 391 Loss: 0.944 | Acc: 66.567% (6646/9984)\n",
            "78 391 Loss: 0.943 | Acc: 66.614% (6736/10112)\n",
            "79 391 Loss: 0.941 | Acc: 66.650% (6825/10240)\n",
            "80 391 Loss: 0.939 | Acc: 66.667% (6912/10368)\n",
            "81 391 Loss: 0.939 | Acc: 66.654% (6996/10496)\n",
            "82 391 Loss: 0.937 | Acc: 66.736% (7090/10624)\n",
            "83 391 Loss: 0.937 | Acc: 66.750% (7177/10752)\n",
            "84 391 Loss: 0.934 | Acc: 66.847% (7273/10880)\n",
            "85 391 Loss: 0.934 | Acc: 66.860% (7360/11008)\n",
            "86 391 Loss: 0.934 | Acc: 66.891% (7449/11136)\n",
            "87 391 Loss: 0.933 | Acc: 66.948% (7541/11264)\n",
            "88 391 Loss: 0.931 | Acc: 67.047% (7638/11392)\n",
            "89 391 Loss: 0.930 | Acc: 67.092% (7729/11520)\n",
            "90 391 Loss: 0.927 | Acc: 67.162% (7823/11648)\n",
            "91 391 Loss: 0.927 | Acc: 67.145% (7907/11776)\n",
            "92 391 Loss: 0.926 | Acc: 67.204% (8000/11904)\n",
            "93 391 Loss: 0.925 | Acc: 67.229% (8089/12032)\n",
            "94 391 Loss: 0.926 | Acc: 67.212% (8173/12160)\n",
            "95 391 Loss: 0.925 | Acc: 67.204% (8258/12288)\n",
            "96 391 Loss: 0.925 | Acc: 67.268% (8352/12416)\n",
            "97 391 Loss: 0.924 | Acc: 67.275% (8439/12544)\n",
            "98 391 Loss: 0.921 | Acc: 67.385% (8539/12672)\n",
            "99 391 Loss: 0.921 | Acc: 67.438% (8632/12800)\n",
            "100 391 Loss: 0.921 | Acc: 67.435% (8718/12928)\n",
            "101 391 Loss: 0.922 | Acc: 67.417% (8802/13056)\n",
            "102 391 Loss: 0.921 | Acc: 67.438% (8891/13184)\n",
            "103 391 Loss: 0.921 | Acc: 67.428% (8976/13312)\n",
            "104 391 Loss: 0.921 | Acc: 67.470% (9068/13440)\n",
            "105 391 Loss: 0.921 | Acc: 67.453% (9152/13568)\n",
            "106 391 Loss: 0.920 | Acc: 67.501% (9245/13696)\n",
            "107 391 Loss: 0.918 | Acc: 67.556% (9339/13824)\n",
            "108 391 Loss: 0.917 | Acc: 67.560% (9426/13952)\n",
            "109 391 Loss: 0.916 | Acc: 67.628% (9522/14080)\n",
            "110 391 Loss: 0.917 | Acc: 67.589% (9603/14208)\n",
            "111 391 Loss: 0.916 | Acc: 67.690% (9704/14336)\n",
            "112 391 Loss: 0.917 | Acc: 67.665% (9787/14464)\n",
            "113 391 Loss: 0.918 | Acc: 67.633% (9869/14592)\n",
            "114 391 Loss: 0.917 | Acc: 67.677% (9962/14720)\n",
            "115 391 Loss: 0.917 | Acc: 67.679% (10049/14848)\n",
            "116 391 Loss: 0.916 | Acc: 67.715% (10141/14976)\n",
            "117 391 Loss: 0.915 | Acc: 67.783% (10238/15104)\n",
            "118 391 Loss: 0.914 | Acc: 67.785% (10325/15232)\n",
            "119 391 Loss: 0.914 | Acc: 67.812% (10416/15360)\n",
            "120 391 Loss: 0.913 | Acc: 67.859% (10510/15488)\n",
            "121 391 Loss: 0.913 | Acc: 67.860% (10597/15616)\n",
            "122 391 Loss: 0.912 | Acc: 67.905% (10691/15744)\n",
            "123 391 Loss: 0.911 | Acc: 67.925% (10781/15872)\n",
            "124 391 Loss: 0.911 | Acc: 67.987% (10878/16000)\n",
            "125 391 Loss: 0.910 | Acc: 68.000% (10967/16128)\n",
            "126 391 Loss: 0.909 | Acc: 68.043% (11061/16256)\n",
            "127 391 Loss: 0.907 | Acc: 68.103% (11158/16384)\n",
            "128 391 Loss: 0.906 | Acc: 68.114% (11247/16512)\n",
            "129 391 Loss: 0.904 | Acc: 68.185% (11346/16640)\n",
            "130 391 Loss: 0.905 | Acc: 68.207% (11437/16768)\n",
            "131 391 Loss: 0.903 | Acc: 68.223% (11527/16896)\n",
            "132 391 Loss: 0.903 | Acc: 68.210% (11612/17024)\n",
            "133 391 Loss: 0.903 | Acc: 68.202% (11698/17152)\n",
            "134 391 Loss: 0.902 | Acc: 68.264% (11796/17280)\n",
            "135 391 Loss: 0.901 | Acc: 68.313% (11892/17408)\n",
            "136 391 Loss: 0.902 | Acc: 68.317% (11980/17536)\n",
            "137 391 Loss: 0.901 | Acc: 68.303% (12065/17664)\n",
            "138 391 Loss: 0.900 | Acc: 68.368% (12164/17792)\n",
            "139 391 Loss: 0.900 | Acc: 68.359% (12250/17920)\n",
            "140 391 Loss: 0.898 | Acc: 68.401% (12345/18048)\n",
            "141 391 Loss: 0.898 | Acc: 68.403% (12433/18176)\n",
            "142 391 Loss: 0.898 | Acc: 68.395% (12519/18304)\n",
            "143 391 Loss: 0.896 | Acc: 68.473% (12621/18432)\n",
            "144 391 Loss: 0.897 | Acc: 68.394% (12694/18560)\n",
            "145 391 Loss: 0.896 | Acc: 68.440% (12790/18688)\n",
            "146 391 Loss: 0.896 | Acc: 68.436% (12877/18816)\n",
            "147 391 Loss: 0.897 | Acc: 68.433% (12964/18944)\n",
            "148 391 Loss: 0.896 | Acc: 68.446% (13054/19072)\n",
            "149 391 Loss: 0.897 | Acc: 68.406% (13134/19200)\n",
            "150 391 Loss: 0.896 | Acc: 68.414% (13223/19328)\n",
            "151 391 Loss: 0.897 | Acc: 68.375% (13303/19456)\n",
            "152 391 Loss: 0.897 | Acc: 68.367% (13389/19584)\n",
            "153 391 Loss: 0.897 | Acc: 68.375% (13478/19712)\n",
            "154 391 Loss: 0.896 | Acc: 68.407% (13572/19840)\n",
            "155 391 Loss: 0.895 | Acc: 68.455% (13669/19968)\n",
            "156 391 Loss: 0.896 | Acc: 68.446% (13755/20096)\n",
            "157 391 Loss: 0.896 | Acc: 68.473% (13848/20224)\n",
            "158 391 Loss: 0.896 | Acc: 68.470% (13935/20352)\n",
            "159 391 Loss: 0.897 | Acc: 68.442% (14017/20480)\n",
            "160 391 Loss: 0.896 | Acc: 68.464% (14109/20608)\n",
            "161 391 Loss: 0.896 | Acc: 68.441% (14192/20736)\n",
            "162 391 Loss: 0.896 | Acc: 68.429% (14277/20864)\n",
            "163 391 Loss: 0.896 | Acc: 68.397% (14358/20992)\n",
            "164 391 Loss: 0.895 | Acc: 68.404% (14447/21120)\n",
            "165 391 Loss: 0.896 | Acc: 68.373% (14528/21248)\n",
            "166 391 Loss: 0.896 | Acc: 68.362% (14613/21376)\n",
            "167 391 Loss: 0.896 | Acc: 68.322% (14692/21504)\n",
            "168 391 Loss: 0.897 | Acc: 68.274% (14769/21632)\n",
            "169 391 Loss: 0.897 | Acc: 68.267% (14855/21760)\n",
            "170 391 Loss: 0.896 | Acc: 68.307% (14951/21888)\n",
            "171 391 Loss: 0.895 | Acc: 68.341% (15046/22016)\n",
            "172 391 Loss: 0.896 | Acc: 68.307% (15126/22144)\n",
            "173 391 Loss: 0.896 | Acc: 68.323% (15217/22272)\n",
            "174 391 Loss: 0.895 | Acc: 68.321% (15304/22400)\n",
            "175 391 Loss: 0.896 | Acc: 68.302% (15387/22528)\n",
            "176 391 Loss: 0.895 | Acc: 68.335% (15482/22656)\n",
            "177 391 Loss: 0.894 | Acc: 68.386% (15581/22784)\n",
            "178 391 Loss: 0.894 | Acc: 68.383% (15668/22912)\n",
            "179 391 Loss: 0.893 | Acc: 68.420% (15764/23040)\n",
            "180 391 Loss: 0.892 | Acc: 68.465% (15862/23168)\n",
            "181 391 Loss: 0.891 | Acc: 68.514% (15961/23296)\n",
            "182 391 Loss: 0.891 | Acc: 68.537% (16054/23424)\n",
            "183 391 Loss: 0.891 | Acc: 68.559% (16147/23552)\n",
            "184 391 Loss: 0.890 | Acc: 68.602% (16245/23680)\n",
            "185 391 Loss: 0.890 | Acc: 68.628% (16339/23808)\n",
            "186 391 Loss: 0.889 | Acc: 68.671% (16437/23936)\n",
            "187 391 Loss: 0.888 | Acc: 68.663% (16523/24064)\n",
            "188 391 Loss: 0.888 | Acc: 68.680% (16615/24192)\n",
            "189 391 Loss: 0.888 | Acc: 68.680% (16703/24320)\n",
            "190 391 Loss: 0.888 | Acc: 68.676% (16790/24448)\n",
            "191 391 Loss: 0.887 | Acc: 68.689% (16881/24576)\n",
            "192 391 Loss: 0.886 | Acc: 68.710% (16974/24704)\n",
            "193 391 Loss: 0.886 | Acc: 68.722% (17065/24832)\n",
            "194 391 Loss: 0.886 | Acc: 68.722% (17153/24960)\n",
            "195 391 Loss: 0.884 | Acc: 68.802% (17261/25088)\n",
            "196 391 Loss: 0.883 | Acc: 68.857% (17363/25216)\n",
            "197 391 Loss: 0.882 | Acc: 68.884% (17458/25344)\n",
            "198 391 Loss: 0.881 | Acc: 68.895% (17549/25472)\n",
            "199 391 Loss: 0.881 | Acc: 68.895% (17637/25600)\n",
            "200 391 Loss: 0.881 | Acc: 68.917% (17731/25728)\n",
            "201 391 Loss: 0.880 | Acc: 68.932% (17823/25856)\n",
            "202 391 Loss: 0.880 | Acc: 68.942% (17914/25984)\n",
            "203 391 Loss: 0.879 | Acc: 68.934% (18000/26112)\n",
            "204 391 Loss: 0.878 | Acc: 68.971% (18098/26240)\n",
            "205 391 Loss: 0.878 | Acc: 68.974% (18187/26368)\n",
            "206 391 Loss: 0.878 | Acc: 68.984% (18278/26496)\n",
            "207 391 Loss: 0.878 | Acc: 68.979% (18365/26624)\n",
            "208 391 Loss: 0.877 | Acc: 68.997% (18458/26752)\n",
            "209 391 Loss: 0.877 | Acc: 68.988% (18544/26880)\n",
            "210 391 Loss: 0.878 | Acc: 68.954% (18623/27008)\n",
            "211 391 Loss: 0.877 | Acc: 68.990% (18721/27136)\n",
            "212 391 Loss: 0.876 | Acc: 69.029% (18820/27264)\n",
            "213 391 Loss: 0.876 | Acc: 69.046% (18913/27392)\n",
            "214 391 Loss: 0.875 | Acc: 69.052% (19003/27520)\n",
            "215 391 Loss: 0.876 | Acc: 69.061% (19094/27648)\n",
            "216 391 Loss: 0.875 | Acc: 69.070% (19185/27776)\n",
            "217 391 Loss: 0.874 | Acc: 69.105% (19283/27904)\n",
            "218 391 Loss: 0.874 | Acc: 69.117% (19375/28032)\n",
            "219 391 Loss: 0.873 | Acc: 69.151% (19473/28160)\n",
            "220 391 Loss: 0.873 | Acc: 69.157% (19563/28288)\n",
            "221 391 Loss: 0.872 | Acc: 69.183% (19659/28416)\n",
            "222 391 Loss: 0.871 | Acc: 69.195% (19751/28544)\n",
            "223 391 Loss: 0.871 | Acc: 69.224% (19848/28672)\n",
            "224 391 Loss: 0.871 | Acc: 69.219% (19935/28800)\n",
            "225 391 Loss: 0.870 | Acc: 69.237% (20029/28928)\n",
            "226 391 Loss: 0.869 | Acc: 69.252% (20122/29056)\n",
            "227 391 Loss: 0.869 | Acc: 69.267% (20215/29184)\n",
            "228 391 Loss: 0.869 | Acc: 69.275% (20306/29312)\n",
            "229 391 Loss: 0.868 | Acc: 69.307% (20404/29440)\n",
            "230 391 Loss: 0.867 | Acc: 69.366% (20510/29568)\n",
            "231 391 Loss: 0.866 | Acc: 69.423% (20616/29696)\n",
            "232 391 Loss: 0.866 | Acc: 69.400% (20698/29824)\n",
            "233 391 Loss: 0.866 | Acc: 69.374% (20779/29952)\n",
            "234 391 Loss: 0.865 | Acc: 69.418% (20881/30080)\n",
            "235 391 Loss: 0.865 | Acc: 69.419% (20970/30208)\n",
            "236 391 Loss: 0.866 | Acc: 69.413% (21057/30336)\n",
            "237 391 Loss: 0.866 | Acc: 69.407% (21144/30464)\n",
            "238 391 Loss: 0.865 | Acc: 69.423% (21238/30592)\n",
            "239 391 Loss: 0.866 | Acc: 69.388% (21316/30720)\n",
            "240 391 Loss: 0.866 | Acc: 69.385% (21404/30848)\n",
            "241 391 Loss: 0.866 | Acc: 69.389% (21494/30976)\n",
            "242 391 Loss: 0.865 | Acc: 69.416% (21591/31104)\n",
            "243 391 Loss: 0.865 | Acc: 69.416% (21680/31232)\n",
            "244 391 Loss: 0.864 | Acc: 69.432% (21774/31360)\n",
            "245 391 Loss: 0.863 | Acc: 69.455% (21870/31488)\n",
            "246 391 Loss: 0.864 | Acc: 69.436% (21953/31616)\n",
            "247 391 Loss: 0.864 | Acc: 69.459% (22049/31744)\n",
            "248 391 Loss: 0.863 | Acc: 69.465% (22140/31872)\n",
            "249 391 Loss: 0.862 | Acc: 69.497% (22239/32000)\n",
            "250 391 Loss: 0.862 | Acc: 69.528% (22338/32128)\n",
            "251 391 Loss: 0.862 | Acc: 69.537% (22430/32256)\n",
            "252 391 Loss: 0.861 | Acc: 69.553% (22524/32384)\n",
            "253 391 Loss: 0.861 | Acc: 69.568% (22618/32512)\n",
            "254 391 Loss: 0.860 | Acc: 69.568% (22707/32640)\n",
            "255 391 Loss: 0.859 | Acc: 69.614% (22811/32768)\n",
            "256 391 Loss: 0.859 | Acc: 69.635% (22907/32896)\n",
            "257 391 Loss: 0.858 | Acc: 69.668% (23007/33024)\n",
            "258 391 Loss: 0.858 | Acc: 69.691% (23104/33152)\n",
            "259 391 Loss: 0.858 | Acc: 69.724% (23204/33280)\n",
            "260 391 Loss: 0.857 | Acc: 69.720% (23292/33408)\n",
            "261 391 Loss: 0.857 | Acc: 69.710% (23378/33536)\n",
            "262 391 Loss: 0.857 | Acc: 69.715% (23469/33664)\n",
            "263 391 Loss: 0.856 | Acc: 69.741% (23567/33792)\n",
            "264 391 Loss: 0.856 | Acc: 69.758% (23662/33920)\n",
            "265 391 Loss: 0.855 | Acc: 69.784% (23760/34048)\n",
            "266 391 Loss: 0.854 | Acc: 69.830% (23865/34176)\n",
            "267 391 Loss: 0.855 | Acc: 69.814% (23949/34304)\n",
            "268 391 Loss: 0.855 | Acc: 69.822% (24041/34432)\n",
            "269 391 Loss: 0.855 | Acc: 69.818% (24129/34560)\n",
            "270 391 Loss: 0.854 | Acc: 69.828% (24222/34688)\n",
            "271 391 Loss: 0.853 | Acc: 69.864% (24324/34816)\n",
            "272 391 Loss: 0.852 | Acc: 69.892% (24423/34944)\n",
            "273 391 Loss: 0.852 | Acc: 69.913% (24520/35072)\n",
            "274 391 Loss: 0.852 | Acc: 69.946% (24621/35200)\n",
            "275 391 Loss: 0.851 | Acc: 69.953% (24713/35328)\n",
            "276 391 Loss: 0.851 | Acc: 69.980% (24812/35456)\n",
            "277 391 Loss: 0.851 | Acc: 69.978% (24901/35584)\n",
            "278 391 Loss: 0.851 | Acc: 69.974% (24989/35712)\n",
            "279 391 Loss: 0.851 | Acc: 69.978% (25080/35840)\n",
            "280 391 Loss: 0.850 | Acc: 69.993% (25175/35968)\n",
            "281 391 Loss: 0.850 | Acc: 70.013% (25272/36096)\n",
            "282 391 Loss: 0.849 | Acc: 70.017% (25363/36224)\n",
            "283 391 Loss: 0.849 | Acc: 70.029% (25457/36352)\n",
            "284 391 Loss: 0.849 | Acc: 70.019% (25543/36480)\n",
            "285 391 Loss: 0.849 | Acc: 70.039% (25640/36608)\n",
            "286 391 Loss: 0.848 | Acc: 70.057% (25736/36736)\n",
            "287 391 Loss: 0.847 | Acc: 70.068% (25830/36864)\n",
            "288 391 Loss: 0.847 | Acc: 70.083% (25925/36992)\n",
            "289 391 Loss: 0.846 | Acc: 70.110% (26025/37120)\n",
            "290 391 Loss: 0.846 | Acc: 70.117% (26117/37248)\n",
            "291 391 Loss: 0.846 | Acc: 70.112% (26205/37376)\n",
            "292 391 Loss: 0.846 | Acc: 70.121% (26298/37504)\n",
            "293 391 Loss: 0.846 | Acc: 70.111% (26384/37632)\n",
            "294 391 Loss: 0.846 | Acc: 70.111% (26474/37760)\n",
            "295 391 Loss: 0.846 | Acc: 70.122% (26568/37888)\n",
            "296 391 Loss: 0.845 | Acc: 70.139% (26664/38016)\n",
            "297 391 Loss: 0.845 | Acc: 70.145% (26756/38144)\n",
            "298 391 Loss: 0.845 | Acc: 70.153% (26849/38272)\n",
            "299 391 Loss: 0.845 | Acc: 70.161% (26942/38400)\n",
            "300 391 Loss: 0.844 | Acc: 70.178% (27038/38528)\n",
            "301 391 Loss: 0.844 | Acc: 70.196% (27135/38656)\n",
            "302 391 Loss: 0.844 | Acc: 70.186% (27221/38784)\n",
            "303 391 Loss: 0.843 | Acc: 70.192% (27313/38912)\n",
            "304 391 Loss: 0.843 | Acc: 70.202% (27407/39040)\n",
            "305 391 Loss: 0.843 | Acc: 70.205% (27498/39168)\n",
            "306 391 Loss: 0.842 | Acc: 70.236% (27600/39296)\n",
            "307 391 Loss: 0.842 | Acc: 70.252% (27696/39424)\n",
            "308 391 Loss: 0.841 | Acc: 70.277% (27796/39552)\n",
            "309 391 Loss: 0.841 | Acc: 70.287% (27890/39680)\n",
            "310 391 Loss: 0.840 | Acc: 70.292% (27982/39808)\n",
            "311 391 Loss: 0.840 | Acc: 70.297% (28074/39936)\n",
            "312 391 Loss: 0.839 | Acc: 70.317% (28172/40064)\n",
            "313 391 Loss: 0.839 | Acc: 70.327% (28266/40192)\n",
            "314 391 Loss: 0.839 | Acc: 70.327% (28356/40320)\n",
            "315 391 Loss: 0.838 | Acc: 70.345% (28453/40448)\n",
            "316 391 Loss: 0.838 | Acc: 70.359% (28549/40576)\n",
            "317 391 Loss: 0.838 | Acc: 70.347% (28634/40704)\n",
            "318 391 Loss: 0.838 | Acc: 70.342% (28722/40832)\n",
            "319 391 Loss: 0.838 | Acc: 70.327% (28806/40960)\n",
            "320 391 Loss: 0.838 | Acc: 70.334% (28899/41088)\n",
            "321 391 Loss: 0.838 | Acc: 70.327% (28986/41216)\n",
            "322 391 Loss: 0.838 | Acc: 70.349% (29085/41344)\n",
            "323 391 Loss: 0.838 | Acc: 70.366% (29182/41472)\n",
            "324 391 Loss: 0.838 | Acc: 70.375% (29276/41600)\n",
            "325 391 Loss: 0.838 | Acc: 70.358% (29359/41728)\n",
            "326 391 Loss: 0.837 | Acc: 70.382% (29459/41856)\n",
            "327 391 Loss: 0.837 | Acc: 70.379% (29548/41984)\n",
            "328 391 Loss: 0.837 | Acc: 70.391% (29643/42112)\n",
            "329 391 Loss: 0.836 | Acc: 70.398% (29736/42240)\n",
            "330 391 Loss: 0.837 | Acc: 70.393% (29824/42368)\n",
            "331 391 Loss: 0.836 | Acc: 70.416% (29924/42496)\n",
            "332 391 Loss: 0.836 | Acc: 70.425% (30018/42624)\n",
            "333 391 Loss: 0.836 | Acc: 70.434% (30112/42752)\n",
            "334 391 Loss: 0.835 | Acc: 70.466% (30216/42880)\n",
            "335 391 Loss: 0.835 | Acc: 70.466% (30306/43008)\n",
            "336 391 Loss: 0.835 | Acc: 70.470% (30398/43136)\n",
            "337 391 Loss: 0.835 | Acc: 70.486% (30495/43264)\n",
            "338 391 Loss: 0.835 | Acc: 70.478% (30582/43392)\n",
            "339 391 Loss: 0.834 | Acc: 70.485% (30675/43520)\n",
            "340 391 Loss: 0.834 | Acc: 70.503% (30773/43648)\n",
            "341 391 Loss: 0.834 | Acc: 70.507% (30865/43776)\n",
            "342 391 Loss: 0.833 | Acc: 70.515% (30959/43904)\n",
            "343 391 Loss: 0.833 | Acc: 70.524% (31053/44032)\n",
            "344 391 Loss: 0.833 | Acc: 70.521% (31142/44160)\n",
            "345 391 Loss: 0.833 | Acc: 70.534% (31238/44288)\n",
            "346 391 Loss: 0.832 | Acc: 70.560% (31340/44416)\n",
            "347 391 Loss: 0.832 | Acc: 70.557% (31429/44544)\n",
            "348 391 Loss: 0.831 | Acc: 70.574% (31527/44672)\n",
            "349 391 Loss: 0.831 | Acc: 70.576% (31618/44800)\n",
            "350 391 Loss: 0.830 | Acc: 70.606% (31722/44928)\n",
            "351 391 Loss: 0.830 | Acc: 70.630% (31823/45056)\n",
            "352 391 Loss: 0.830 | Acc: 70.642% (31919/45184)\n",
            "353 391 Loss: 0.829 | Acc: 70.670% (32022/45312)\n",
            "354 391 Loss: 0.829 | Acc: 70.687% (32120/45440)\n",
            "355 391 Loss: 0.829 | Acc: 70.679% (32207/45568)\n",
            "356 391 Loss: 0.828 | Acc: 70.687% (32301/45696)\n",
            "357 391 Loss: 0.828 | Acc: 70.694% (32395/45824)\n",
            "358 391 Loss: 0.827 | Acc: 70.715% (32495/45952)\n",
            "359 391 Loss: 0.827 | Acc: 70.736% (32595/46080)\n",
            "360 391 Loss: 0.827 | Acc: 70.735% (32685/46208)\n",
            "361 391 Loss: 0.827 | Acc: 70.744% (32780/46336)\n",
            "362 391 Loss: 0.826 | Acc: 70.747% (32872/46464)\n",
            "363 391 Loss: 0.826 | Acc: 70.759% (32968/46592)\n",
            "364 391 Loss: 0.825 | Acc: 70.775% (33066/46720)\n",
            "365 391 Loss: 0.825 | Acc: 70.786% (33162/46848)\n",
            "366 391 Loss: 0.825 | Acc: 70.804% (33261/46976)\n",
            "367 391 Loss: 0.824 | Acc: 70.811% (33355/47104)\n",
            "368 391 Loss: 0.824 | Acc: 70.819% (33449/47232)\n",
            "369 391 Loss: 0.824 | Acc: 70.823% (33542/47360)\n",
            "370 391 Loss: 0.824 | Acc: 70.845% (33643/47488)\n",
            "371 391 Loss: 0.823 | Acc: 70.859% (33740/47616)\n",
            "372 391 Loss: 0.823 | Acc: 70.851% (33827/47744)\n",
            "373 391 Loss: 0.823 | Acc: 70.851% (33918/47872)\n",
            "374 391 Loss: 0.823 | Acc: 70.873% (34019/48000)\n",
            "375 391 Loss: 0.822 | Acc: 70.886% (34116/48128)\n",
            "376 391 Loss: 0.822 | Acc: 70.893% (34210/48256)\n",
            "377 391 Loss: 0.822 | Acc: 70.902% (34305/48384)\n",
            "378 391 Loss: 0.821 | Acc: 70.923% (34406/48512)\n",
            "379 391 Loss: 0.822 | Acc: 70.900% (34486/48640)\n",
            "380 391 Loss: 0.822 | Acc: 70.909% (34581/48768)\n",
            "381 391 Loss: 0.821 | Acc: 70.916% (34675/48896)\n",
            "382 391 Loss: 0.821 | Acc: 70.920% (34768/49024)\n",
            "383 391 Loss: 0.821 | Acc: 70.919% (34858/49152)\n",
            "384 391 Loss: 0.821 | Acc: 70.938% (34958/49280)\n",
            "385 391 Loss: 0.821 | Acc: 70.938% (35049/49408)\n",
            "386 391 Loss: 0.821 | Acc: 70.942% (35142/49536)\n",
            "387 391 Loss: 0.821 | Acc: 70.927% (35225/49664)\n",
            "388 391 Loss: 0.821 | Acc: 70.927% (35316/49792)\n",
            "389 391 Loss: 0.821 | Acc: 70.913% (35400/49920)\n",
            "390 391 Loss: 0.821 | Acc: 70.918% (35459/50000)\n",
            "0 100 Loss: 0.936 | Acc: 72.000% (72/100)\n",
            "1 100 Loss: 0.898 | Acc: 72.000% (144/200)\n",
            "2 100 Loss: 0.925 | Acc: 69.333% (208/300)\n",
            "3 100 Loss: 0.913 | Acc: 69.250% (277/400)\n",
            "4 100 Loss: 0.910 | Acc: 69.200% (346/500)\n",
            "5 100 Loss: 0.880 | Acc: 69.500% (417/600)\n",
            "6 100 Loss: 0.902 | Acc: 69.143% (484/700)\n",
            "7 100 Loss: 0.937 | Acc: 68.250% (546/800)\n",
            "8 100 Loss: 0.938 | Acc: 69.000% (621/900)\n",
            "9 100 Loss: 0.932 | Acc: 69.200% (692/1000)\n",
            "10 100 Loss: 0.925 | Acc: 69.000% (759/1100)\n",
            "11 100 Loss: 0.929 | Acc: 68.917% (827/1200)\n",
            "12 100 Loss: 0.929 | Acc: 68.308% (888/1300)\n",
            "13 100 Loss: 0.921 | Acc: 68.571% (960/1400)\n",
            "14 100 Loss: 0.924 | Acc: 68.333% (1025/1500)\n",
            "15 100 Loss: 0.915 | Acc: 68.562% (1097/1600)\n",
            "16 100 Loss: 0.916 | Acc: 68.412% (1163/1700)\n",
            "17 100 Loss: 0.918 | Acc: 68.278% (1229/1800)\n",
            "18 100 Loss: 0.919 | Acc: 68.263% (1297/1900)\n",
            "19 100 Loss: 0.932 | Acc: 68.050% (1361/2000)\n",
            "20 100 Loss: 0.930 | Acc: 68.048% (1429/2100)\n",
            "21 100 Loss: 0.933 | Acc: 67.955% (1495/2200)\n",
            "22 100 Loss: 0.929 | Acc: 68.174% (1568/2300)\n",
            "23 100 Loss: 0.926 | Acc: 68.208% (1637/2400)\n",
            "24 100 Loss: 0.925 | Acc: 68.120% (1703/2500)\n",
            "25 100 Loss: 0.937 | Acc: 67.923% (1766/2600)\n",
            "26 100 Loss: 0.929 | Acc: 68.074% (1838/2700)\n",
            "27 100 Loss: 0.934 | Acc: 67.964% (1903/2800)\n",
            "28 100 Loss: 0.934 | Acc: 67.931% (1970/2900)\n",
            "29 100 Loss: 0.931 | Acc: 68.100% (2043/3000)\n",
            "30 100 Loss: 0.927 | Acc: 68.258% (2116/3100)\n",
            "31 100 Loss: 0.927 | Acc: 68.375% (2188/3200)\n",
            "32 100 Loss: 0.926 | Acc: 68.424% (2258/3300)\n",
            "33 100 Loss: 0.925 | Acc: 68.412% (2326/3400)\n",
            "34 100 Loss: 0.930 | Acc: 68.171% (2386/3500)\n",
            "35 100 Loss: 0.924 | Acc: 68.361% (2461/3600)\n",
            "36 100 Loss: 0.929 | Acc: 68.189% (2523/3700)\n",
            "37 100 Loss: 0.932 | Acc: 67.947% (2582/3800)\n",
            "38 100 Loss: 0.930 | Acc: 67.949% (2650/3900)\n",
            "39 100 Loss: 0.930 | Acc: 67.975% (2719/4000)\n",
            "40 100 Loss: 0.935 | Acc: 67.976% (2787/4100)\n",
            "41 100 Loss: 0.935 | Acc: 68.024% (2857/4200)\n",
            "42 100 Loss: 0.932 | Acc: 68.140% (2930/4300)\n",
            "43 100 Loss: 0.930 | Acc: 68.205% (3001/4400)\n",
            "44 100 Loss: 0.926 | Acc: 68.378% (3077/4500)\n",
            "45 100 Loss: 0.927 | Acc: 68.326% (3143/4600)\n",
            "46 100 Loss: 0.924 | Acc: 68.362% (3213/4700)\n",
            "47 100 Loss: 0.929 | Acc: 68.250% (3276/4800)\n",
            "48 100 Loss: 0.927 | Acc: 68.286% (3346/4900)\n",
            "49 100 Loss: 0.930 | Acc: 68.200% (3410/5000)\n",
            "50 100 Loss: 0.931 | Acc: 68.216% (3479/5100)\n",
            "51 100 Loss: 0.931 | Acc: 68.250% (3549/5200)\n",
            "52 100 Loss: 0.930 | Acc: 68.208% (3615/5300)\n",
            "53 100 Loss: 0.932 | Acc: 68.222% (3684/5400)\n",
            "54 100 Loss: 0.931 | Acc: 68.236% (3753/5500)\n",
            "55 100 Loss: 0.931 | Acc: 68.286% (3824/5600)\n",
            "56 100 Loss: 0.933 | Acc: 68.333% (3895/5700)\n",
            "57 100 Loss: 0.931 | Acc: 68.466% (3971/5800)\n",
            "58 100 Loss: 0.934 | Acc: 68.322% (4031/5900)\n",
            "59 100 Loss: 0.933 | Acc: 68.333% (4100/6000)\n",
            "60 100 Loss: 0.933 | Acc: 68.328% (4168/6100)\n",
            "61 100 Loss: 0.931 | Acc: 68.290% (4234/6200)\n",
            "62 100 Loss: 0.929 | Acc: 68.302% (4303/6300)\n",
            "63 100 Loss: 0.928 | Acc: 68.297% (4371/6400)\n",
            "64 100 Loss: 0.930 | Acc: 68.262% (4437/6500)\n",
            "65 100 Loss: 0.931 | Acc: 68.197% (4501/6600)\n",
            "66 100 Loss: 0.929 | Acc: 68.209% (4570/6700)\n",
            "67 100 Loss: 0.931 | Acc: 68.162% (4635/6800)\n",
            "68 100 Loss: 0.930 | Acc: 68.188% (4705/6900)\n",
            "69 100 Loss: 0.932 | Acc: 68.143% (4770/7000)\n",
            "70 100 Loss: 0.934 | Acc: 68.127% (4837/7100)\n",
            "71 100 Loss: 0.933 | Acc: 68.111% (4904/7200)\n",
            "72 100 Loss: 0.931 | Acc: 68.137% (4974/7300)\n",
            "73 100 Loss: 0.928 | Acc: 68.176% (5045/7400)\n",
            "74 100 Loss: 0.929 | Acc: 68.200% (5115/7500)\n",
            "75 100 Loss: 0.928 | Acc: 68.171% (5181/7600)\n",
            "76 100 Loss: 0.928 | Acc: 68.169% (5249/7700)\n",
            "77 100 Loss: 0.930 | Acc: 68.077% (5310/7800)\n",
            "78 100 Loss: 0.930 | Acc: 68.063% (5377/7900)\n",
            "79 100 Loss: 0.930 | Acc: 68.075% (5446/8000)\n",
            "80 100 Loss: 0.927 | Acc: 68.086% (5515/8100)\n",
            "81 100 Loss: 0.926 | Acc: 68.085% (5583/8200)\n",
            "82 100 Loss: 0.927 | Acc: 68.120% (5654/8300)\n",
            "83 100 Loss: 0.929 | Acc: 68.024% (5714/8400)\n",
            "84 100 Loss: 0.931 | Acc: 67.965% (5777/8500)\n",
            "85 100 Loss: 0.932 | Acc: 67.930% (5842/8600)\n",
            "86 100 Loss: 0.933 | Acc: 67.885% (5906/8700)\n",
            "87 100 Loss: 0.934 | Acc: 67.818% (5968/8800)\n",
            "88 100 Loss: 0.934 | Acc: 67.865% (6040/8900)\n",
            "89 100 Loss: 0.936 | Acc: 67.833% (6105/9000)\n",
            "90 100 Loss: 0.935 | Acc: 67.736% (6164/9100)\n",
            "91 100 Loss: 0.933 | Acc: 67.761% (6234/9200)\n",
            "92 100 Loss: 0.936 | Acc: 67.656% (6292/9300)\n",
            "93 100 Loss: 0.936 | Acc: 67.649% (6359/9400)\n",
            "94 100 Loss: 0.935 | Acc: 67.674% (6429/9500)\n",
            "95 100 Loss: 0.933 | Acc: 67.750% (6504/9600)\n",
            "96 100 Loss: 0.931 | Acc: 67.804% (6577/9700)\n",
            "97 100 Loss: 0.935 | Acc: 67.704% (6635/9800)\n",
            "98 100 Loss: 0.934 | Acc: 67.697% (6702/9900)\n",
            "99 100 Loss: 0.935 | Acc: 67.670% (6767/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzhr9EhC1UN8",
        "colab_type": "text"
      },
      "source": [
        "# GoogleNet ( Inception V2 ) \n",
        "\n",
        "1.   Replaced one 5x5 Kernal with 2 (3x3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1jBVN5U1Y4N",
        "colab_type": "code",
        "outputId": "494fa926-c4fe-4cc9-a85b-852cb6d3ac56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "'''GoogLeNet with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class InceptionV2(nn.Module):\n",
        "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
        "        super(InceptionV2, self).__init__()\n",
        "        # 1x1 conv branch\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(n1x1),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # 1x1 conv -> 3x3 conv branch\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n3x3red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n3x3),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        #print(n5x5)\n",
        "        # 1x1 conv -> 5x5 conv branch\n",
        "        self.b3 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n5x5red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # 3x3 pool -> 1x1 conv branch\n",
        "        self.b4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
        "            nn.BatchNorm2d(pool_planes),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y1 = self.b1(x)\n",
        "        y2 = self.b2(x)\n",
        "        y3 = self.b3(x)\n",
        "        y4 = self.b4(x)\n",
        "        '''print(y1.size())\n",
        "        print(y2.size())\n",
        "        print(y3.size())\n",
        "        print(y4.size())'''\n",
        "        return torch.cat([y1,y2,y3,y4], 1)\n",
        "class GoogLeNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GoogLeNet2, self).__init__()\n",
        "        self.pre_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.a3 = InceptionV2(192,  64,  96, 128, 16, 32, 32)\n",
        "        self.b3 = InceptionV2(256, 128, 128, 192, 32, 96, 64)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        self.a4 = InceptionV2(480, 192,  96, 208, 16,  48,  64)\n",
        "        self.b4 = InceptionV2(512, 160, 112, 224, 24,  64,  64)\n",
        "        self.c4 = InceptionV2(512, 128, 128, 256, 24,  64,  64)\n",
        "        self.d4 = InceptionV2(512, 112, 144, 288, 32,  64,  64)\n",
        "        self.e4 = InceptionV2(528, 256, 160, 320, 32, 128, 128)\n",
        "\n",
        "        self.a5 = InceptionV2(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.b5 = InceptionV2(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.linear = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pre_layers(x)\n",
        "        out = self.a3(out)\n",
        "        out = self.b3(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.a4(out)\n",
        "        out = self.b4(out)\n",
        "        out = self.c4(out)\n",
        "        out = self.d4(out)\n",
        "        out = self.e4(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.a5(out)\n",
        "        out = self.b5(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "'''def test():\n",
        "    net = GoogLeNet2()\n",
        "    x = torch.randn(1,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "    print(y)\n",
        "\n",
        "test()'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n",
            "tensor([[-0.1829, -0.3134, -0.3924, -0.1104, -0.1087, -0.1274,  0.1873, -0.0997,\n",
            "         -0.2290,  0.2572]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJaXVzyG46sv",
        "colab_type": "text"
      },
      "source": [
        "# Training GoogLeNet2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-X9hc9c4-ew",
        "colab_type": "code",
        "outputId": "69ad5a30-289b-4156-ab56-d1760e5e825c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "#from utils import progress_bar\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "net2 = GoogLeNet2()\n",
        "net2 = net2.to(device)\n",
        "if device == 'cuda':\n",
        "    net2 = torch.nn.DataParallel(net2)\n",
        "    cudnn.benchmark = True\n",
        "#Loss and Optimization\n",
        "criterion = nn.CrossEntropyLoss() # It is useful when training a classification problem with `C` classes.\n",
        "optimizer = optim.SGD(net2.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Training\n",
        "def train2(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net2.train() # Sets the model in training mode\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #print(inputs.size())\n",
        "        #print(targets)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net2(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() # UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "#Testing \n",
        "def test2(epoch):\n",
        "    global best_acc\n",
        "    net2.eval() # Sets the model in evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net2(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Building model..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40nzk9wd5PF4",
        "colab_type": "code",
        "outputId": "a23f6b21-81a0-418d-9a7e-5bb7ab1bed69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch+2):\n",
        "    train2(epoch)\n",
        "    test2(epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 2.321 | Acc: 7.031% (9/128)\n",
            "1 391 Loss: 2.297 | Acc: 12.891% (33/256)\n",
            "2 391 Loss: 2.295 | Acc: 15.365% (59/384)\n",
            "3 391 Loss: 2.250 | Acc: 16.602% (85/512)\n",
            "4 391 Loss: 2.226 | Acc: 18.594% (119/640)\n",
            "5 391 Loss: 2.218 | Acc: 19.141% (147/768)\n",
            "6 391 Loss: 2.170 | Acc: 20.759% (186/896)\n",
            "7 391 Loss: 2.170 | Acc: 20.215% (207/1024)\n",
            "8 391 Loss: 2.171 | Acc: 20.833% (240/1152)\n",
            "9 391 Loss: 2.176 | Acc: 20.859% (267/1280)\n",
            "10 391 Loss: 2.170 | Acc: 21.094% (297/1408)\n",
            "11 391 Loss: 2.151 | Acc: 21.484% (330/1536)\n",
            "12 391 Loss: 2.145 | Acc: 21.875% (364/1664)\n",
            "13 391 Loss: 2.129 | Acc: 22.210% (398/1792)\n",
            "14 391 Loss: 2.120 | Acc: 22.760% (437/1920)\n",
            "15 391 Loss: 2.100 | Acc: 23.389% (479/2048)\n",
            "16 391 Loss: 2.085 | Acc: 24.219% (527/2176)\n",
            "17 391 Loss: 2.077 | Acc: 24.653% (568/2304)\n",
            "18 391 Loss: 2.067 | Acc: 25.164% (612/2432)\n",
            "19 391 Loss: 2.059 | Acc: 25.469% (652/2560)\n",
            "20 391 Loss: 2.053 | Acc: 25.893% (696/2688)\n",
            "21 391 Loss: 2.043 | Acc: 26.207% (738/2816)\n",
            "22 391 Loss: 2.045 | Acc: 26.053% (767/2944)\n",
            "23 391 Loss: 2.043 | Acc: 26.172% (804/3072)\n",
            "24 391 Loss: 2.043 | Acc: 26.094% (835/3200)\n",
            "25 391 Loss: 2.039 | Acc: 26.352% (877/3328)\n",
            "26 391 Loss: 2.036 | Acc: 26.215% (906/3456)\n",
            "27 391 Loss: 2.030 | Acc: 26.423% (947/3584)\n",
            "28 391 Loss: 2.033 | Acc: 26.239% (974/3712)\n",
            "29 391 Loss: 2.028 | Acc: 26.380% (1013/3840)\n",
            "30 391 Loss: 2.024 | Acc: 26.588% (1055/3968)\n",
            "31 391 Loss: 2.021 | Acc: 26.782% (1097/4096)\n",
            "32 391 Loss: 2.017 | Acc: 26.894% (1136/4224)\n",
            "33 391 Loss: 2.013 | Acc: 27.114% (1180/4352)\n",
            "34 391 Loss: 2.009 | Acc: 27.299% (1223/4480)\n",
            "35 391 Loss: 2.002 | Acc: 27.582% (1271/4608)\n",
            "36 391 Loss: 1.995 | Acc: 27.660% (1310/4736)\n",
            "37 391 Loss: 1.992 | Acc: 27.775% (1351/4864)\n",
            "38 391 Loss: 1.984 | Acc: 28.005% (1398/4992)\n",
            "39 391 Loss: 1.980 | Acc: 28.047% (1436/5120)\n",
            "40 391 Loss: 1.974 | Acc: 28.163% (1478/5248)\n",
            "41 391 Loss: 1.969 | Acc: 28.292% (1521/5376)\n",
            "42 391 Loss: 1.962 | Acc: 28.488% (1568/5504)\n",
            "43 391 Loss: 1.957 | Acc: 28.587% (1610/5632)\n",
            "44 391 Loss: 1.952 | Acc: 28.819% (1660/5760)\n",
            "45 391 Loss: 1.950 | Acc: 28.889% (1701/5888)\n",
            "46 391 Loss: 1.949 | Acc: 28.906% (1739/6016)\n",
            "47 391 Loss: 1.945 | Acc: 29.085% (1787/6144)\n",
            "48 391 Loss: 1.943 | Acc: 29.273% (1836/6272)\n",
            "49 391 Loss: 1.943 | Acc: 29.359% (1879/6400)\n",
            "50 391 Loss: 1.939 | Acc: 29.473% (1924/6528)\n",
            "51 391 Loss: 1.936 | Acc: 29.627% (1972/6656)\n",
            "52 391 Loss: 1.931 | Acc: 29.850% (2025/6784)\n",
            "53 391 Loss: 1.929 | Acc: 29.977% (2072/6912)\n",
            "54 391 Loss: 1.926 | Acc: 30.071% (2117/7040)\n",
            "55 391 Loss: 1.922 | Acc: 30.259% (2169/7168)\n",
            "56 391 Loss: 1.919 | Acc: 30.387% (2217/7296)\n",
            "57 391 Loss: 1.917 | Acc: 30.523% (2266/7424)\n",
            "58 391 Loss: 1.919 | Acc: 30.442% (2299/7552)\n",
            "59 391 Loss: 1.915 | Acc: 30.456% (2339/7680)\n",
            "60 391 Loss: 1.911 | Acc: 30.610% (2390/7808)\n",
            "61 391 Loss: 1.910 | Acc: 30.658% (2433/7936)\n",
            "62 391 Loss: 1.906 | Acc: 30.828% (2486/8064)\n",
            "63 391 Loss: 1.903 | Acc: 30.908% (2532/8192)\n",
            "64 391 Loss: 1.902 | Acc: 30.913% (2572/8320)\n",
            "65 391 Loss: 1.901 | Acc: 30.954% (2615/8448)\n",
            "66 391 Loss: 1.899 | Acc: 31.087% (2666/8576)\n",
            "67 391 Loss: 1.896 | Acc: 31.170% (2713/8704)\n",
            "68 391 Loss: 1.894 | Acc: 31.239% (2759/8832)\n",
            "69 391 Loss: 1.891 | Acc: 31.462% (2819/8960)\n",
            "70 391 Loss: 1.888 | Acc: 31.646% (2876/9088)\n",
            "71 391 Loss: 1.887 | Acc: 31.695% (2921/9216)\n",
            "72 391 Loss: 1.885 | Acc: 31.753% (2967/9344)\n",
            "73 391 Loss: 1.880 | Acc: 31.852% (3017/9472)\n",
            "74 391 Loss: 1.878 | Acc: 31.906% (3063/9600)\n",
            "75 391 Loss: 1.877 | Acc: 31.939% (3107/9728)\n",
            "76 391 Loss: 1.872 | Acc: 32.021% (3156/9856)\n",
            "77 391 Loss: 1.869 | Acc: 32.131% (3208/9984)\n",
            "78 391 Loss: 1.864 | Acc: 32.338% (3270/10112)\n",
            "79 391 Loss: 1.860 | Acc: 32.451% (3323/10240)\n",
            "80 391 Loss: 1.855 | Acc: 32.610% (3381/10368)\n",
            "81 391 Loss: 1.852 | Acc: 32.689% (3431/10496)\n",
            "82 391 Loss: 1.848 | Acc: 32.765% (3481/10624)\n",
            "83 391 Loss: 1.843 | Acc: 32.878% (3535/10752)\n",
            "84 391 Loss: 1.843 | Acc: 32.858% (3575/10880)\n",
            "85 391 Loss: 1.840 | Acc: 32.994% (3632/11008)\n",
            "86 391 Loss: 1.838 | Acc: 33.118% (3688/11136)\n",
            "87 391 Loss: 1.834 | Acc: 33.336% (3755/11264)\n",
            "88 391 Loss: 1.830 | Acc: 33.427% (3808/11392)\n",
            "89 391 Loss: 1.826 | Acc: 33.542% (3864/11520)\n",
            "90 391 Loss: 1.823 | Acc: 33.602% (3914/11648)\n",
            "91 391 Loss: 1.821 | Acc: 33.577% (3954/11776)\n",
            "92 391 Loss: 1.818 | Acc: 33.703% (4012/11904)\n",
            "93 391 Loss: 1.815 | Acc: 33.860% (4074/12032)\n",
            "94 391 Loss: 1.810 | Acc: 34.013% (4136/12160)\n",
            "95 391 Loss: 1.807 | Acc: 34.074% (4187/12288)\n",
            "96 391 Loss: 1.804 | Acc: 34.206% (4247/12416)\n",
            "97 391 Loss: 1.802 | Acc: 34.287% (4301/12544)\n",
            "98 391 Loss: 1.799 | Acc: 34.343% (4352/12672)\n",
            "99 391 Loss: 1.798 | Acc: 34.398% (4403/12800)\n",
            "100 391 Loss: 1.798 | Acc: 34.437% (4452/12928)\n",
            "101 391 Loss: 1.795 | Acc: 34.528% (4508/13056)\n",
            "102 391 Loss: 1.791 | Acc: 34.587% (4560/13184)\n",
            "103 391 Loss: 1.789 | Acc: 34.668% (4615/13312)\n",
            "104 391 Loss: 1.787 | Acc: 34.665% (4659/13440)\n",
            "105 391 Loss: 1.783 | Acc: 34.795% (4721/13568)\n",
            "106 391 Loss: 1.781 | Acc: 34.893% (4779/13696)\n",
            "107 391 Loss: 1.777 | Acc: 35.055% (4846/13824)\n",
            "108 391 Loss: 1.775 | Acc: 35.171% (4907/13952)\n",
            "109 391 Loss: 1.773 | Acc: 35.185% (4954/14080)\n",
            "110 391 Loss: 1.772 | Acc: 35.255% (5009/14208)\n",
            "111 391 Loss: 1.769 | Acc: 35.310% (5062/14336)\n",
            "112 391 Loss: 1.766 | Acc: 35.371% (5116/14464)\n",
            "113 391 Loss: 1.763 | Acc: 35.458% (5174/14592)\n",
            "114 391 Loss: 1.761 | Acc: 35.510% (5227/14720)\n",
            "115 391 Loss: 1.758 | Acc: 35.634% (5291/14848)\n",
            "116 391 Loss: 1.756 | Acc: 35.724% (5350/14976)\n",
            "117 391 Loss: 1.754 | Acc: 35.779% (5404/15104)\n",
            "118 391 Loss: 1.752 | Acc: 35.865% (5463/15232)\n",
            "119 391 Loss: 1.752 | Acc: 35.918% (5517/15360)\n",
            "120 391 Loss: 1.748 | Acc: 36.041% (5582/15488)\n",
            "121 391 Loss: 1.746 | Acc: 36.085% (5635/15616)\n",
            "122 391 Loss: 1.743 | Acc: 36.211% (5701/15744)\n",
            "123 391 Loss: 1.740 | Acc: 36.316% (5764/15872)\n",
            "124 391 Loss: 1.737 | Acc: 36.406% (5825/16000)\n",
            "125 391 Loss: 1.736 | Acc: 36.452% (5879/16128)\n",
            "126 391 Loss: 1.734 | Acc: 36.596% (5949/16256)\n",
            "127 391 Loss: 1.732 | Acc: 36.646% (6004/16384)\n",
            "128 391 Loss: 1.730 | Acc: 36.695% (6059/16512)\n",
            "129 391 Loss: 1.728 | Acc: 36.833% (6129/16640)\n",
            "130 391 Loss: 1.725 | Acc: 36.933% (6193/16768)\n",
            "131 391 Loss: 1.722 | Acc: 37.086% (6266/16896)\n",
            "132 391 Loss: 1.719 | Acc: 37.183% (6330/17024)\n",
            "133 391 Loss: 1.716 | Acc: 37.249% (6389/17152)\n",
            "134 391 Loss: 1.714 | Acc: 37.338% (6452/17280)\n",
            "135 391 Loss: 1.712 | Acc: 37.420% (6514/17408)\n",
            "136 391 Loss: 1.710 | Acc: 37.443% (6566/17536)\n",
            "137 391 Loss: 1.707 | Acc: 37.540% (6631/17664)\n",
            "138 391 Loss: 1.705 | Acc: 37.624% (6694/17792)\n",
            "139 391 Loss: 1.703 | Acc: 37.734% (6762/17920)\n",
            "140 391 Loss: 1.699 | Acc: 37.877% (6836/18048)\n",
            "141 391 Loss: 1.698 | Acc: 37.940% (6896/18176)\n",
            "142 391 Loss: 1.695 | Acc: 38.019% (6959/18304)\n",
            "143 391 Loss: 1.693 | Acc: 38.129% (7028/18432)\n",
            "144 391 Loss: 1.690 | Acc: 38.200% (7090/18560)\n",
            "145 391 Loss: 1.688 | Acc: 38.319% (7161/18688)\n",
            "146 391 Loss: 1.685 | Acc: 38.457% (7236/18816)\n",
            "147 391 Loss: 1.682 | Acc: 38.535% (7300/18944)\n",
            "148 391 Loss: 1.679 | Acc: 38.664% (7374/19072)\n",
            "149 391 Loss: 1.678 | Acc: 38.677% (7426/19200)\n",
            "150 391 Loss: 1.676 | Acc: 38.773% (7494/19328)\n",
            "151 391 Loss: 1.674 | Acc: 38.857% (7560/19456)\n",
            "152 391 Loss: 1.670 | Acc: 38.981% (7634/19584)\n",
            "153 391 Loss: 1.668 | Acc: 39.083% (7704/19712)\n",
            "154 391 Loss: 1.667 | Acc: 39.123% (7762/19840)\n",
            "155 391 Loss: 1.666 | Acc: 39.178% (7823/19968)\n",
            "156 391 Loss: 1.663 | Acc: 39.281% (7894/20096)\n",
            "157 391 Loss: 1.661 | Acc: 39.334% (7955/20224)\n",
            "158 391 Loss: 1.660 | Acc: 39.372% (8013/20352)\n",
            "159 391 Loss: 1.656 | Acc: 39.473% (8084/20480)\n",
            "160 391 Loss: 1.654 | Acc: 39.514% (8143/20608)\n",
            "161 391 Loss: 1.652 | Acc: 39.579% (8207/20736)\n",
            "162 391 Loss: 1.651 | Acc: 39.638% (8270/20864)\n",
            "163 391 Loss: 1.649 | Acc: 39.687% (8331/20992)\n",
            "164 391 Loss: 1.647 | Acc: 39.777% (8401/21120)\n",
            "165 391 Loss: 1.645 | Acc: 39.858% (8469/21248)\n",
            "166 391 Loss: 1.644 | Acc: 39.914% (8532/21376)\n",
            "167 391 Loss: 1.643 | Acc: 39.969% (8595/21504)\n",
            "168 391 Loss: 1.641 | Acc: 40.056% (8665/21632)\n",
            "169 391 Loss: 1.639 | Acc: 40.101% (8726/21760)\n",
            "170 391 Loss: 1.637 | Acc: 40.191% (8797/21888)\n",
            "171 391 Loss: 1.635 | Acc: 40.280% (8868/22016)\n",
            "172 391 Loss: 1.634 | Acc: 40.336% (8932/22144)\n",
            "173 391 Loss: 1.632 | Acc: 40.405% (8999/22272)\n",
            "174 391 Loss: 1.631 | Acc: 40.451% (9061/22400)\n",
            "175 391 Loss: 1.628 | Acc: 40.527% (9130/22528)\n",
            "176 391 Loss: 1.626 | Acc: 40.590% (9196/22656)\n",
            "177 391 Loss: 1.625 | Acc: 40.643% (9260/22784)\n",
            "178 391 Loss: 1.622 | Acc: 40.743% (9335/22912)\n",
            "179 391 Loss: 1.620 | Acc: 40.838% (9409/23040)\n",
            "180 391 Loss: 1.617 | Acc: 40.970% (9492/23168)\n",
            "181 391 Loss: 1.614 | Acc: 41.063% (9566/23296)\n",
            "182 391 Loss: 1.612 | Acc: 41.137% (9636/23424)\n",
            "183 391 Loss: 1.610 | Acc: 41.241% (9713/23552)\n",
            "184 391 Loss: 1.607 | Acc: 41.347% (9791/23680)\n",
            "185 391 Loss: 1.605 | Acc: 41.419% (9861/23808)\n",
            "186 391 Loss: 1.604 | Acc: 41.486% (9930/23936)\n",
            "187 391 Loss: 1.603 | Acc: 41.543% (9997/24064)\n",
            "188 391 Loss: 1.600 | Acc: 41.638% (10073/24192)\n",
            "189 391 Loss: 1.598 | Acc: 41.752% (10154/24320)\n",
            "190 391 Loss: 1.597 | Acc: 41.783% (10215/24448)\n",
            "191 391 Loss: 1.596 | Acc: 41.809% (10275/24576)\n",
            "192 391 Loss: 1.595 | Acc: 41.872% (10344/24704)\n",
            "193 391 Loss: 1.592 | Acc: 41.990% (10427/24832)\n",
            "194 391 Loss: 1.590 | Acc: 42.035% (10492/24960)\n",
            "195 391 Loss: 1.589 | Acc: 42.060% (10552/25088)\n",
            "196 391 Loss: 1.588 | Acc: 42.120% (10621/25216)\n",
            "197 391 Loss: 1.586 | Acc: 42.188% (10692/25344)\n",
            "198 391 Loss: 1.585 | Acc: 42.231% (10757/25472)\n",
            "199 391 Loss: 1.583 | Acc: 42.309% (10831/25600)\n",
            "200 391 Loss: 1.580 | Acc: 42.397% (10908/25728)\n",
            "201 391 Loss: 1.578 | Acc: 42.478% (10983/25856)\n",
            "202 391 Loss: 1.576 | Acc: 42.538% (11053/25984)\n",
            "203 391 Loss: 1.575 | Acc: 42.609% (11126/26112)\n",
            "204 391 Loss: 1.573 | Acc: 42.683% (11200/26240)\n",
            "205 391 Loss: 1.572 | Acc: 42.741% (11270/26368)\n",
            "206 391 Loss: 1.570 | Acc: 42.818% (11345/26496)\n",
            "207 391 Loss: 1.569 | Acc: 42.864% (11412/26624)\n",
            "208 391 Loss: 1.567 | Acc: 42.943% (11488/26752)\n",
            "209 391 Loss: 1.566 | Acc: 43.010% (11561/26880)\n",
            "210 391 Loss: 1.564 | Acc: 43.087% (11637/27008)\n",
            "211 391 Loss: 1.562 | Acc: 43.153% (11710/27136)\n",
            "212 391 Loss: 1.561 | Acc: 43.181% (11773/27264)\n",
            "213 391 Loss: 1.559 | Acc: 43.246% (11846/27392)\n",
            "214 391 Loss: 1.556 | Acc: 43.343% (11928/27520)\n",
            "215 391 Loss: 1.554 | Acc: 43.406% (12001/27648)\n",
            "216 391 Loss: 1.553 | Acc: 43.476% (12076/27776)\n",
            "217 391 Loss: 1.551 | Acc: 43.556% (12154/27904)\n",
            "218 391 Loss: 1.550 | Acc: 43.582% (12217/28032)\n",
            "219 391 Loss: 1.547 | Acc: 43.675% (12299/28160)\n",
            "220 391 Loss: 1.545 | Acc: 43.754% (12377/28288)\n",
            "221 391 Loss: 1.544 | Acc: 43.799% (12446/28416)\n",
            "222 391 Loss: 1.543 | Acc: 43.848% (12516/28544)\n",
            "223 391 Loss: 1.542 | Acc: 43.914% (12591/28672)\n",
            "224 391 Loss: 1.540 | Acc: 43.979% (12666/28800)\n",
            "225 391 Loss: 1.539 | Acc: 44.023% (12735/28928)\n",
            "226 391 Loss: 1.538 | Acc: 44.067% (12804/29056)\n",
            "227 391 Loss: 1.536 | Acc: 44.113% (12874/29184)\n",
            "228 391 Loss: 1.535 | Acc: 44.159% (12944/29312)\n",
            "229 391 Loss: 1.533 | Acc: 44.239% (13024/29440)\n",
            "230 391 Loss: 1.531 | Acc: 44.291% (13096/29568)\n",
            "231 391 Loss: 1.529 | Acc: 44.356% (13172/29696)\n",
            "232 391 Loss: 1.527 | Acc: 44.421% (13248/29824)\n",
            "233 391 Loss: 1.525 | Acc: 44.478% (13322/29952)\n",
            "234 391 Loss: 1.523 | Acc: 44.541% (13398/30080)\n",
            "235 391 Loss: 1.522 | Acc: 44.588% (13469/30208)\n",
            "236 391 Loss: 1.520 | Acc: 44.650% (13545/30336)\n",
            "237 391 Loss: 1.518 | Acc: 44.735% (13628/30464)\n",
            "238 391 Loss: 1.517 | Acc: 44.786% (13701/30592)\n",
            "239 391 Loss: 1.515 | Acc: 44.840% (13775/30720)\n",
            "240 391 Loss: 1.513 | Acc: 44.914% (13855/30848)\n",
            "241 391 Loss: 1.511 | Acc: 44.977% (13932/30976)\n",
            "242 391 Loss: 1.510 | Acc: 45.026% (14005/31104)\n",
            "243 391 Loss: 1.509 | Acc: 45.095% (14084/31232)\n",
            "244 391 Loss: 1.509 | Acc: 45.124% (14151/31360)\n",
            "245 391 Loss: 1.507 | Acc: 45.179% (14226/31488)\n",
            "246 391 Loss: 1.506 | Acc: 45.252% (14307/31616)\n",
            "247 391 Loss: 1.504 | Acc: 45.322% (14387/31744)\n",
            "248 391 Loss: 1.502 | Acc: 45.397% (14469/31872)\n",
            "249 391 Loss: 1.500 | Acc: 45.456% (14546/32000)\n",
            "250 391 Loss: 1.499 | Acc: 45.530% (14628/32128)\n",
            "251 391 Loss: 1.497 | Acc: 45.588% (14705/32256)\n",
            "252 391 Loss: 1.496 | Acc: 45.655% (14785/32384)\n",
            "253 391 Loss: 1.495 | Acc: 45.663% (14846/32512)\n",
            "254 391 Loss: 1.494 | Acc: 45.708% (14919/32640)\n",
            "255 391 Loss: 1.493 | Acc: 45.728% (14984/32768)\n",
            "256 391 Loss: 1.491 | Acc: 45.778% (15059/32896)\n",
            "257 391 Loss: 1.490 | Acc: 45.824% (15133/33024)\n",
            "258 391 Loss: 1.489 | Acc: 45.861% (15204/33152)\n",
            "259 391 Loss: 1.487 | Acc: 45.919% (15282/33280)\n",
            "260 391 Loss: 1.486 | Acc: 45.971% (15358/33408)\n",
            "261 391 Loss: 1.485 | Acc: 45.963% (15414/33536)\n",
            "262 391 Loss: 1.484 | Acc: 46.002% (15486/33664)\n",
            "263 391 Loss: 1.483 | Acc: 46.055% (15563/33792)\n",
            "264 391 Loss: 1.481 | Acc: 46.126% (15646/33920)\n",
            "265 391 Loss: 1.480 | Acc: 46.188% (15726/34048)\n",
            "266 391 Loss: 1.478 | Acc: 46.252% (15807/34176)\n",
            "267 391 Loss: 1.478 | Acc: 46.251% (15866/34304)\n",
            "268 391 Loss: 1.476 | Acc: 46.344% (15957/34432)\n",
            "269 391 Loss: 1.476 | Acc: 46.374% (16027/34560)\n",
            "270 391 Loss: 1.474 | Acc: 46.422% (16103/34688)\n",
            "271 391 Loss: 1.473 | Acc: 46.473% (16180/34816)\n",
            "272 391 Loss: 1.471 | Acc: 46.546% (16265/34944)\n",
            "273 391 Loss: 1.470 | Acc: 46.601% (16344/35072)\n",
            "274 391 Loss: 1.468 | Acc: 46.668% (16427/35200)\n",
            "275 391 Loss: 1.466 | Acc: 46.731% (16509/35328)\n",
            "276 391 Loss: 1.465 | Acc: 46.779% (16586/35456)\n",
            "277 391 Loss: 1.463 | Acc: 46.836% (16666/35584)\n",
            "278 391 Loss: 1.463 | Acc: 46.869% (16738/35712)\n",
            "279 391 Loss: 1.462 | Acc: 46.922% (16817/35840)\n",
            "280 391 Loss: 1.460 | Acc: 46.978% (16897/35968)\n",
            "281 391 Loss: 1.458 | Acc: 47.025% (16974/36096)\n",
            "282 391 Loss: 1.457 | Acc: 47.079% (17054/36224)\n",
            "283 391 Loss: 1.455 | Acc: 47.114% (17127/36352)\n",
            "284 391 Loss: 1.454 | Acc: 47.171% (17208/36480)\n",
            "285 391 Loss: 1.453 | Acc: 47.233% (17291/36608)\n",
            "286 391 Loss: 1.452 | Acc: 47.259% (17361/36736)\n",
            "287 391 Loss: 1.450 | Acc: 47.333% (17449/36864)\n",
            "288 391 Loss: 1.449 | Acc: 47.375% (17525/36992)\n",
            "289 391 Loss: 1.448 | Acc: 47.395% (17593/37120)\n",
            "290 391 Loss: 1.446 | Acc: 47.436% (17669/37248)\n",
            "291 391 Loss: 1.445 | Acc: 47.472% (17743/37376)\n",
            "292 391 Loss: 1.445 | Acc: 47.486% (17809/37504)\n",
            "293 391 Loss: 1.443 | Acc: 47.529% (17886/37632)\n",
            "294 391 Loss: 1.443 | Acc: 47.564% (17960/37760)\n",
            "295 391 Loss: 1.442 | Acc: 47.606% (18037/37888)\n",
            "296 391 Loss: 1.441 | Acc: 47.625% (18105/38016)\n",
            "297 391 Loss: 1.440 | Acc: 47.646% (18174/38144)\n",
            "298 391 Loss: 1.439 | Acc: 47.682% (18249/38272)\n",
            "299 391 Loss: 1.438 | Acc: 47.732% (18329/38400)\n",
            "300 391 Loss: 1.437 | Acc: 47.778% (18408/38528)\n",
            "301 391 Loss: 1.436 | Acc: 47.793% (18475/38656)\n",
            "302 391 Loss: 1.435 | Acc: 47.852% (18559/38784)\n",
            "303 391 Loss: 1.434 | Acc: 47.900% (18639/38912)\n",
            "304 391 Loss: 1.433 | Acc: 47.953% (18721/39040)\n",
            "305 391 Loss: 1.432 | Acc: 47.986% (18795/39168)\n",
            "306 391 Loss: 1.430 | Acc: 48.025% (18872/39296)\n",
            "307 391 Loss: 1.429 | Acc: 48.082% (18956/39424)\n",
            "308 391 Loss: 1.428 | Acc: 48.111% (19029/39552)\n",
            "309 391 Loss: 1.426 | Acc: 48.158% (19109/39680)\n",
            "310 391 Loss: 1.425 | Acc: 48.186% (19182/39808)\n",
            "311 391 Loss: 1.425 | Acc: 48.212% (19254/39936)\n",
            "312 391 Loss: 1.424 | Acc: 48.260% (19335/40064)\n",
            "313 391 Loss: 1.422 | Acc: 48.318% (19420/40192)\n",
            "314 391 Loss: 1.420 | Acc: 48.373% (19504/40320)\n",
            "315 391 Loss: 1.419 | Acc: 48.418% (19584/40448)\n",
            "316 391 Loss: 1.418 | Acc: 48.455% (19661/40576)\n",
            "317 391 Loss: 1.417 | Acc: 48.482% (19734/40704)\n",
            "318 391 Loss: 1.417 | Acc: 48.486% (19798/40832)\n",
            "319 391 Loss: 1.415 | Acc: 48.550% (19886/40960)\n",
            "320 391 Loss: 1.414 | Acc: 48.593% (19966/41088)\n",
            "321 391 Loss: 1.412 | Acc: 48.658% (20055/41216)\n",
            "322 391 Loss: 1.411 | Acc: 48.716% (20141/41344)\n",
            "323 391 Loss: 1.409 | Acc: 48.794% (20236/41472)\n",
            "324 391 Loss: 1.408 | Acc: 48.846% (20320/41600)\n",
            "325 391 Loss: 1.406 | Acc: 48.895% (20403/41728)\n",
            "326 391 Loss: 1.405 | Acc: 48.939% (20484/41856)\n",
            "327 391 Loss: 1.404 | Acc: 48.988% (20567/41984)\n",
            "328 391 Loss: 1.403 | Acc: 49.019% (20643/42112)\n",
            "329 391 Loss: 1.401 | Acc: 49.081% (20732/42240)\n",
            "330 391 Loss: 1.400 | Acc: 49.115% (20809/42368)\n",
            "331 391 Loss: 1.399 | Acc: 49.141% (20883/42496)\n",
            "332 391 Loss: 1.399 | Acc: 49.169% (20958/42624)\n",
            "333 391 Loss: 1.398 | Acc: 49.207% (21037/42752)\n",
            "334 391 Loss: 1.396 | Acc: 49.275% (21129/42880)\n",
            "335 391 Loss: 1.395 | Acc: 49.323% (21213/43008)\n",
            "336 391 Loss: 1.394 | Acc: 49.360% (21292/43136)\n",
            "337 391 Loss: 1.392 | Acc: 49.415% (21379/43264)\n",
            "338 391 Loss: 1.391 | Acc: 49.488% (21474/43392)\n",
            "339 391 Loss: 1.390 | Acc: 49.522% (21552/43520)\n",
            "340 391 Loss: 1.389 | Acc: 49.558% (21631/43648)\n",
            "341 391 Loss: 1.388 | Acc: 49.616% (21720/43776)\n",
            "342 391 Loss: 1.386 | Acc: 49.658% (21802/43904)\n",
            "343 391 Loss: 1.385 | Acc: 49.691% (21880/44032)\n",
            "344 391 Loss: 1.385 | Acc: 49.737% (21964/44160)\n",
            "345 391 Loss: 1.383 | Acc: 49.783% (22048/44288)\n",
            "346 391 Loss: 1.382 | Acc: 49.813% (22125/44416)\n",
            "347 391 Loss: 1.382 | Acc: 49.829% (22196/44544)\n",
            "348 391 Loss: 1.381 | Acc: 49.884% (22284/44672)\n",
            "349 391 Loss: 1.379 | Acc: 49.949% (22377/44800)\n",
            "350 391 Loss: 1.378 | Acc: 49.991% (22460/44928)\n",
            "351 391 Loss: 1.378 | Acc: 50.009% (22532/45056)\n",
            "352 391 Loss: 1.377 | Acc: 50.053% (22616/45184)\n",
            "353 391 Loss: 1.376 | Acc: 50.104% (22703/45312)\n",
            "354 391 Loss: 1.375 | Acc: 50.145% (22786/45440)\n",
            "355 391 Loss: 1.374 | Acc: 50.176% (22864/45568)\n",
            "356 391 Loss: 1.373 | Acc: 50.197% (22938/45696)\n",
            "357 391 Loss: 1.372 | Acc: 50.216% (23011/45824)\n",
            "358 391 Loss: 1.371 | Acc: 50.259% (23095/45952)\n",
            "359 391 Loss: 1.370 | Acc: 50.315% (23185/46080)\n",
            "360 391 Loss: 1.368 | Acc: 50.364% (23272/46208)\n",
            "361 391 Loss: 1.367 | Acc: 50.388% (23348/46336)\n",
            "362 391 Loss: 1.367 | Acc: 50.424% (23429/46464)\n",
            "363 391 Loss: 1.365 | Acc: 50.466% (23513/46592)\n",
            "364 391 Loss: 1.365 | Acc: 50.503% (23595/46720)\n",
            "365 391 Loss: 1.363 | Acc: 50.557% (23685/46848)\n",
            "366 391 Loss: 1.362 | Acc: 50.607% (23773/46976)\n",
            "367 391 Loss: 1.361 | Acc: 50.628% (23848/47104)\n",
            "368 391 Loss: 1.360 | Acc: 50.667% (23931/47232)\n",
            "369 391 Loss: 1.359 | Acc: 50.688% (24006/47360)\n",
            "370 391 Loss: 1.359 | Acc: 50.716% (24084/47488)\n",
            "371 391 Loss: 1.357 | Acc: 50.767% (24173/47616)\n",
            "372 391 Loss: 1.356 | Acc: 50.804% (24256/47744)\n",
            "373 391 Loss: 1.356 | Acc: 50.813% (24325/47872)\n",
            "374 391 Loss: 1.355 | Acc: 50.844% (24405/48000)\n",
            "375 391 Loss: 1.354 | Acc: 50.864% (24480/48128)\n",
            "376 391 Loss: 1.353 | Acc: 50.895% (24560/48256)\n",
            "377 391 Loss: 1.352 | Acc: 50.905% (24630/48384)\n",
            "378 391 Loss: 1.351 | Acc: 50.944% (24714/48512)\n",
            "379 391 Loss: 1.350 | Acc: 50.987% (24800/48640)\n",
            "380 391 Loss: 1.349 | Acc: 51.019% (24881/48768)\n",
            "381 391 Loss: 1.349 | Acc: 51.041% (24957/48896)\n",
            "382 391 Loss: 1.348 | Acc: 51.067% (25035/49024)\n",
            "383 391 Loss: 1.347 | Acc: 51.113% (25123/49152)\n",
            "384 391 Loss: 1.347 | Acc: 51.126% (25195/49280)\n",
            "385 391 Loss: 1.345 | Acc: 51.172% (25283/49408)\n",
            "386 391 Loss: 1.345 | Acc: 51.215% (25370/49536)\n",
            "387 391 Loss: 1.344 | Acc: 51.240% (25448/49664)\n",
            "388 391 Loss: 1.343 | Acc: 51.259% (25523/49792)\n",
            "389 391 Loss: 1.343 | Acc: 51.296% (25607/49920)\n",
            "390 391 Loss: 1.341 | Acc: 51.326% (25663/50000)\n",
            "0 100 Loss: 1.302 | Acc: 58.000% (58/100)\n",
            "1 100 Loss: 1.291 | Acc: 56.000% (112/200)\n",
            "2 100 Loss: 1.240 | Acc: 58.333% (175/300)\n",
            "3 100 Loss: 1.243 | Acc: 56.500% (226/400)\n",
            "4 100 Loss: 1.288 | Acc: 55.400% (277/500)\n",
            "5 100 Loss: 1.251 | Acc: 56.500% (339/600)\n",
            "6 100 Loss: 1.236 | Acc: 56.429% (395/700)\n",
            "7 100 Loss: 1.257 | Acc: 55.250% (442/800)\n",
            "8 100 Loss: 1.278 | Acc: 54.778% (493/900)\n",
            "9 100 Loss: 1.259 | Acc: 55.100% (551/1000)\n",
            "10 100 Loss: 1.230 | Acc: 56.636% (623/1100)\n",
            "11 100 Loss: 1.219 | Acc: 56.500% (678/1200)\n",
            "12 100 Loss: 1.238 | Acc: 56.077% (729/1300)\n",
            "13 100 Loss: 1.233 | Acc: 56.143% (786/1400)\n",
            "14 100 Loss: 1.229 | Acc: 56.800% (852/1500)\n",
            "15 100 Loss: 1.234 | Acc: 56.938% (911/1600)\n",
            "16 100 Loss: 1.238 | Acc: 56.941% (968/1700)\n",
            "17 100 Loss: 1.244 | Acc: 56.611% (1019/1800)\n",
            "18 100 Loss: 1.246 | Acc: 56.579% (1075/1900)\n",
            "19 100 Loss: 1.251 | Acc: 56.300% (1126/2000)\n",
            "20 100 Loss: 1.255 | Acc: 56.190% (1180/2100)\n",
            "21 100 Loss: 1.251 | Acc: 56.045% (1233/2200)\n",
            "22 100 Loss: 1.259 | Acc: 55.870% (1285/2300)\n",
            "23 100 Loss: 1.260 | Acc: 55.875% (1341/2400)\n",
            "24 100 Loss: 1.260 | Acc: 55.680% (1392/2500)\n",
            "25 100 Loss: 1.279 | Acc: 55.308% (1438/2600)\n",
            "26 100 Loss: 1.279 | Acc: 55.296% (1493/2700)\n",
            "27 100 Loss: 1.281 | Acc: 55.250% (1547/2800)\n",
            "28 100 Loss: 1.281 | Acc: 55.379% (1606/2900)\n",
            "29 100 Loss: 1.276 | Acc: 55.600% (1668/3000)\n",
            "30 100 Loss: 1.274 | Acc: 55.806% (1730/3100)\n",
            "31 100 Loss: 1.272 | Acc: 55.812% (1786/3200)\n",
            "32 100 Loss: 1.274 | Acc: 55.727% (1839/3300)\n",
            "33 100 Loss: 1.276 | Acc: 55.529% (1888/3400)\n",
            "34 100 Loss: 1.276 | Acc: 55.514% (1943/3500)\n",
            "35 100 Loss: 1.275 | Acc: 55.667% (2004/3600)\n",
            "36 100 Loss: 1.276 | Acc: 55.595% (2057/3700)\n",
            "37 100 Loss: 1.276 | Acc: 55.605% (2113/3800)\n",
            "38 100 Loss: 1.275 | Acc: 55.564% (2167/3900)\n",
            "39 100 Loss: 1.273 | Acc: 55.625% (2225/4000)\n",
            "40 100 Loss: 1.275 | Acc: 55.537% (2277/4100)\n",
            "41 100 Loss: 1.273 | Acc: 55.571% (2334/4200)\n",
            "42 100 Loss: 1.269 | Acc: 55.698% (2395/4300)\n",
            "43 100 Loss: 1.266 | Acc: 55.841% (2457/4400)\n",
            "44 100 Loss: 1.263 | Acc: 55.867% (2514/4500)\n",
            "45 100 Loss: 1.268 | Acc: 55.783% (2566/4600)\n",
            "46 100 Loss: 1.267 | Acc: 55.830% (2624/4700)\n",
            "47 100 Loss: 1.263 | Acc: 56.000% (2688/4800)\n",
            "48 100 Loss: 1.258 | Acc: 56.082% (2748/4900)\n",
            "49 100 Loss: 1.263 | Acc: 56.000% (2800/5000)\n",
            "50 100 Loss: 1.261 | Acc: 56.118% (2862/5100)\n",
            "51 100 Loss: 1.260 | Acc: 56.154% (2920/5200)\n",
            "52 100 Loss: 1.257 | Acc: 56.151% (2976/5300)\n",
            "53 100 Loss: 1.260 | Acc: 56.019% (3025/5400)\n",
            "54 100 Loss: 1.258 | Acc: 55.964% (3078/5500)\n",
            "55 100 Loss: 1.259 | Acc: 55.911% (3131/5600)\n",
            "56 100 Loss: 1.262 | Acc: 55.842% (3183/5700)\n",
            "57 100 Loss: 1.260 | Acc: 55.828% (3238/5800)\n",
            "58 100 Loss: 1.264 | Acc: 55.695% (3286/5900)\n",
            "59 100 Loss: 1.268 | Acc: 55.650% (3339/6000)\n",
            "60 100 Loss: 1.270 | Acc: 55.574% (3390/6100)\n",
            "61 100 Loss: 1.273 | Acc: 55.565% (3445/6200)\n",
            "62 100 Loss: 1.274 | Acc: 55.556% (3500/6300)\n",
            "63 100 Loss: 1.273 | Acc: 55.594% (3558/6400)\n",
            "64 100 Loss: 1.275 | Acc: 55.492% (3607/6500)\n",
            "65 100 Loss: 1.275 | Acc: 55.515% (3664/6600)\n",
            "66 100 Loss: 1.276 | Acc: 55.478% (3717/6700)\n",
            "67 100 Loss: 1.280 | Acc: 55.368% (3765/6800)\n",
            "68 100 Loss: 1.276 | Acc: 55.507% (3830/6900)\n",
            "69 100 Loss: 1.280 | Acc: 55.400% (3878/7000)\n",
            "70 100 Loss: 1.280 | Acc: 55.394% (3933/7100)\n",
            "71 100 Loss: 1.279 | Acc: 55.458% (3993/7200)\n",
            "72 100 Loss: 1.279 | Acc: 55.575% (4057/7300)\n",
            "73 100 Loss: 1.275 | Acc: 55.689% (4121/7400)\n",
            "74 100 Loss: 1.278 | Acc: 55.653% (4174/7500)\n",
            "75 100 Loss: 1.275 | Acc: 55.737% (4236/7600)\n",
            "76 100 Loss: 1.274 | Acc: 55.766% (4294/7700)\n",
            "77 100 Loss: 1.276 | Acc: 55.679% (4343/7800)\n",
            "78 100 Loss: 1.276 | Acc: 55.633% (4395/7900)\n",
            "79 100 Loss: 1.277 | Acc: 55.587% (4447/8000)\n",
            "80 100 Loss: 1.275 | Acc: 55.630% (4506/8100)\n",
            "81 100 Loss: 1.277 | Acc: 55.561% (4556/8200)\n",
            "82 100 Loss: 1.279 | Acc: 55.518% (4608/8300)\n",
            "83 100 Loss: 1.284 | Acc: 55.393% (4653/8400)\n",
            "84 100 Loss: 1.283 | Acc: 55.318% (4702/8500)\n",
            "85 100 Loss: 1.282 | Acc: 55.360% (4761/8600)\n",
            "86 100 Loss: 1.283 | Acc: 55.299% (4811/8700)\n",
            "87 100 Loss: 1.284 | Acc: 55.341% (4870/8800)\n",
            "88 100 Loss: 1.285 | Acc: 55.292% (4921/8900)\n",
            "89 100 Loss: 1.285 | Acc: 55.322% (4979/9000)\n",
            "90 100 Loss: 1.285 | Acc: 55.319% (5034/9100)\n",
            "91 100 Loss: 1.283 | Acc: 55.380% (5095/9200)\n",
            "92 100 Loss: 1.284 | Acc: 55.333% (5146/9300)\n",
            "93 100 Loss: 1.285 | Acc: 55.277% (5196/9400)\n",
            "94 100 Loss: 1.286 | Acc: 55.253% (5249/9500)\n",
            "95 100 Loss: 1.283 | Acc: 55.375% (5316/9600)\n",
            "96 100 Loss: 1.281 | Acc: 55.402% (5374/9700)\n",
            "97 100 Loss: 1.285 | Acc: 55.316% (5421/9800)\n",
            "98 100 Loss: 1.286 | Acc: 55.273% (5472/9900)\n",
            "99 100 Loss: 1.285 | Acc: 55.280% (5528/10000)\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 0.913 | Acc: 68.750% (88/128)\n",
            "1 391 Loss: 0.905 | Acc: 69.141% (177/256)\n",
            "2 391 Loss: 0.870 | Acc: 70.312% (270/384)\n",
            "3 391 Loss: 0.904 | Acc: 69.336% (355/512)\n",
            "4 391 Loss: 0.912 | Acc: 67.969% (435/640)\n",
            "5 391 Loss: 0.936 | Acc: 66.797% (513/768)\n",
            "6 391 Loss: 0.928 | Acc: 66.741% (598/896)\n",
            "7 391 Loss: 0.935 | Acc: 66.699% (683/1024)\n",
            "8 391 Loss: 0.962 | Acc: 65.017% (749/1152)\n",
            "9 391 Loss: 0.962 | Acc: 65.000% (832/1280)\n",
            "10 391 Loss: 0.967 | Acc: 64.844% (913/1408)\n",
            "11 391 Loss: 0.966 | Acc: 64.648% (993/1536)\n",
            "12 391 Loss: 0.959 | Acc: 65.024% (1082/1664)\n",
            "13 391 Loss: 0.968 | Acc: 64.565% (1157/1792)\n",
            "14 391 Loss: 0.972 | Acc: 64.531% (1239/1920)\n",
            "15 391 Loss: 0.972 | Acc: 64.551% (1322/2048)\n",
            "16 391 Loss: 0.964 | Acc: 64.890% (1412/2176)\n",
            "17 391 Loss: 0.963 | Acc: 64.844% (1494/2304)\n",
            "18 391 Loss: 0.970 | Acc: 64.803% (1576/2432)\n",
            "19 391 Loss: 0.975 | Acc: 64.609% (1654/2560)\n",
            "20 391 Loss: 0.978 | Acc: 64.472% (1733/2688)\n",
            "21 391 Loss: 0.977 | Acc: 64.418% (1814/2816)\n",
            "22 391 Loss: 0.976 | Acc: 64.538% (1900/2944)\n",
            "23 391 Loss: 0.975 | Acc: 64.681% (1987/3072)\n",
            "24 391 Loss: 0.972 | Acc: 64.875% (2076/3200)\n",
            "25 391 Loss: 0.974 | Acc: 64.814% (2157/3328)\n",
            "26 391 Loss: 0.973 | Acc: 65.075% (2249/3456)\n",
            "27 391 Loss: 0.969 | Acc: 65.402% (2344/3584)\n",
            "28 391 Loss: 0.969 | Acc: 65.517% (2432/3712)\n",
            "29 391 Loss: 0.968 | Acc: 65.495% (2515/3840)\n",
            "30 391 Loss: 0.962 | Acc: 65.675% (2606/3968)\n",
            "31 391 Loss: 0.961 | Acc: 65.625% (2688/4096)\n",
            "32 391 Loss: 0.963 | Acc: 65.601% (2771/4224)\n",
            "33 391 Loss: 0.962 | Acc: 65.510% (2851/4352)\n",
            "34 391 Loss: 0.962 | Acc: 65.558% (2937/4480)\n",
            "35 391 Loss: 0.965 | Acc: 65.473% (3017/4608)\n",
            "36 391 Loss: 0.964 | Acc: 65.477% (3101/4736)\n",
            "37 391 Loss: 0.966 | Acc: 65.563% (3189/4864)\n",
            "38 391 Loss: 0.966 | Acc: 65.405% (3265/4992)\n",
            "39 391 Loss: 0.965 | Acc: 65.488% (3353/5120)\n",
            "40 391 Loss: 0.967 | Acc: 65.587% (3442/5248)\n",
            "41 391 Loss: 0.969 | Acc: 65.532% (3523/5376)\n",
            "42 391 Loss: 0.969 | Acc: 65.625% (3612/5504)\n",
            "43 391 Loss: 0.966 | Acc: 65.678% (3699/5632)\n",
            "44 391 Loss: 0.966 | Acc: 65.573% (3777/5760)\n",
            "45 391 Loss: 0.965 | Acc: 65.625% (3864/5888)\n",
            "46 391 Loss: 0.963 | Acc: 65.691% (3952/6016)\n",
            "47 391 Loss: 0.963 | Acc: 65.641% (4033/6144)\n",
            "48 391 Loss: 0.963 | Acc: 65.673% (4119/6272)\n",
            "49 391 Loss: 0.962 | Acc: 65.688% (4204/6400)\n",
            "50 391 Loss: 0.961 | Acc: 65.748% (4292/6528)\n",
            "51 391 Loss: 0.960 | Acc: 65.805% (4380/6656)\n",
            "52 391 Loss: 0.958 | Acc: 65.949% (4474/6784)\n",
            "53 391 Loss: 0.955 | Acc: 66.001% (4562/6912)\n",
            "54 391 Loss: 0.954 | Acc: 66.023% (4648/7040)\n",
            "55 391 Loss: 0.954 | Acc: 65.932% (4726/7168)\n",
            "56 391 Loss: 0.953 | Acc: 65.995% (4815/7296)\n",
            "57 391 Loss: 0.953 | Acc: 65.962% (4897/7424)\n",
            "58 391 Loss: 0.952 | Acc: 65.983% (4983/7552)\n",
            "59 391 Loss: 0.951 | Acc: 66.016% (5070/7680)\n",
            "60 391 Loss: 0.950 | Acc: 66.035% (5156/7808)\n",
            "61 391 Loss: 0.951 | Acc: 66.079% (5244/7936)\n",
            "62 391 Loss: 0.955 | Acc: 65.923% (5316/8064)\n",
            "63 391 Loss: 0.956 | Acc: 65.918% (5400/8192)\n",
            "64 391 Loss: 0.954 | Acc: 65.986% (5490/8320)\n",
            "65 391 Loss: 0.955 | Acc: 65.885% (5566/8448)\n",
            "66 391 Loss: 0.953 | Acc: 65.975% (5658/8576)\n",
            "67 391 Loss: 0.953 | Acc: 66.016% (5746/8704)\n",
            "68 391 Loss: 0.957 | Acc: 65.863% (5817/8832)\n",
            "69 391 Loss: 0.959 | Acc: 65.792% (5895/8960)\n",
            "70 391 Loss: 0.961 | Acc: 65.592% (5961/9088)\n",
            "71 391 Loss: 0.965 | Acc: 65.495% (6036/9216)\n",
            "72 391 Loss: 0.965 | Acc: 65.475% (6118/9344)\n",
            "73 391 Loss: 0.967 | Acc: 65.393% (6194/9472)\n",
            "74 391 Loss: 0.965 | Acc: 65.469% (6285/9600)\n",
            "75 391 Loss: 0.964 | Acc: 65.481% (6370/9728)\n",
            "76 391 Loss: 0.964 | Acc: 65.513% (6457/9856)\n",
            "77 391 Loss: 0.963 | Acc: 65.485% (6538/9984)\n",
            "78 391 Loss: 0.963 | Acc: 65.526% (6626/10112)\n",
            "79 391 Loss: 0.961 | Acc: 65.596% (6717/10240)\n",
            "80 391 Loss: 0.960 | Acc: 65.635% (6805/10368)\n",
            "81 391 Loss: 0.960 | Acc: 65.644% (6890/10496)\n",
            "82 391 Loss: 0.963 | Acc: 65.606% (6970/10624)\n",
            "83 391 Loss: 0.960 | Acc: 65.727% (7067/10752)\n",
            "84 391 Loss: 0.960 | Acc: 65.781% (7157/10880)\n",
            "85 391 Loss: 0.959 | Acc: 65.779% (7241/11008)\n",
            "86 391 Loss: 0.959 | Acc: 65.769% (7324/11136)\n",
            "87 391 Loss: 0.958 | Acc: 65.829% (7415/11264)\n",
            "88 391 Loss: 0.956 | Acc: 65.906% (7508/11392)\n",
            "89 391 Loss: 0.956 | Acc: 65.981% (7601/11520)\n",
            "90 391 Loss: 0.957 | Acc: 65.986% (7686/11648)\n",
            "91 391 Loss: 0.956 | Acc: 66.024% (7775/11776)\n",
            "92 391 Loss: 0.955 | Acc: 66.079% (7866/11904)\n",
            "93 391 Loss: 0.955 | Acc: 66.099% (7953/12032)\n",
            "94 391 Loss: 0.953 | Acc: 66.143% (8043/12160)\n",
            "95 391 Loss: 0.951 | Acc: 66.284% (8145/12288)\n",
            "96 391 Loss: 0.950 | Acc: 66.350% (8238/12416)\n",
            "97 391 Loss: 0.949 | Acc: 66.390% (8328/12544)\n",
            "98 391 Loss: 0.946 | Acc: 66.438% (8419/12672)\n",
            "99 391 Loss: 0.945 | Acc: 66.469% (8508/12800)\n",
            "100 391 Loss: 0.944 | Acc: 66.507% (8598/12928)\n",
            "101 391 Loss: 0.945 | Acc: 66.491% (8681/13056)\n",
            "102 391 Loss: 0.944 | Acc: 66.512% (8769/13184)\n",
            "103 391 Loss: 0.944 | Acc: 66.526% (8856/13312)\n",
            "104 391 Loss: 0.945 | Acc: 66.540% (8943/13440)\n",
            "105 391 Loss: 0.943 | Acc: 66.620% (9039/13568)\n",
            "106 391 Loss: 0.941 | Acc: 66.684% (9133/13696)\n",
            "107 391 Loss: 0.939 | Acc: 66.732% (9225/13824)\n",
            "108 391 Loss: 0.939 | Acc: 66.736% (9311/13952)\n",
            "109 391 Loss: 0.937 | Acc: 66.754% (9399/14080)\n",
            "110 391 Loss: 0.934 | Acc: 66.864% (9500/14208)\n",
            "111 391 Loss: 0.935 | Acc: 66.895% (9590/14336)\n",
            "112 391 Loss: 0.934 | Acc: 66.918% (9679/14464)\n",
            "113 391 Loss: 0.933 | Acc: 66.893% (9761/14592)\n",
            "114 391 Loss: 0.933 | Acc: 66.895% (9847/14720)\n",
            "115 391 Loss: 0.933 | Acc: 66.905% (9934/14848)\n",
            "116 391 Loss: 0.933 | Acc: 66.914% (10021/14976)\n",
            "117 391 Loss: 0.933 | Acc: 66.943% (10111/15104)\n",
            "118 391 Loss: 0.935 | Acc: 66.899% (10190/15232)\n",
            "119 391 Loss: 0.935 | Acc: 66.947% (10283/15360)\n",
            "120 391 Loss: 0.935 | Acc: 66.936% (10367/15488)\n",
            "121 391 Loss: 0.935 | Acc: 66.919% (10450/15616)\n",
            "122 391 Loss: 0.936 | Acc: 66.851% (10525/15744)\n",
            "123 391 Loss: 0.936 | Acc: 66.910% (10620/15872)\n",
            "124 391 Loss: 0.934 | Acc: 66.950% (10712/16000)\n",
            "125 391 Loss: 0.935 | Acc: 66.939% (10796/16128)\n",
            "126 391 Loss: 0.935 | Acc: 66.954% (10884/16256)\n",
            "127 391 Loss: 0.934 | Acc: 66.962% (10971/16384)\n",
            "128 391 Loss: 0.934 | Acc: 66.957% (11056/16512)\n",
            "129 391 Loss: 0.933 | Acc: 67.043% (11156/16640)\n",
            "130 391 Loss: 0.931 | Acc: 67.056% (11244/16768)\n",
            "131 391 Loss: 0.931 | Acc: 67.081% (11334/16896)\n",
            "132 391 Loss: 0.929 | Acc: 67.129% (11428/17024)\n",
            "133 391 Loss: 0.930 | Acc: 67.106% (11510/17152)\n",
            "134 391 Loss: 0.929 | Acc: 67.147% (11603/17280)\n",
            "135 391 Loss: 0.928 | Acc: 67.188% (11696/17408)\n",
            "136 391 Loss: 0.927 | Acc: 67.216% (11787/17536)\n",
            "137 391 Loss: 0.926 | Acc: 67.221% (11874/17664)\n",
            "138 391 Loss: 0.925 | Acc: 67.272% (11969/17792)\n",
            "139 391 Loss: 0.925 | Acc: 67.271% (12055/17920)\n",
            "140 391 Loss: 0.925 | Acc: 67.298% (12146/18048)\n",
            "141 391 Loss: 0.924 | Acc: 67.303% (12233/18176)\n",
            "142 391 Loss: 0.923 | Acc: 67.351% (12328/18304)\n",
            "143 391 Loss: 0.922 | Acc: 67.377% (12419/18432)\n",
            "144 391 Loss: 0.923 | Acc: 67.376% (12505/18560)\n",
            "145 391 Loss: 0.923 | Acc: 67.385% (12593/18688)\n",
            "146 391 Loss: 0.923 | Acc: 67.379% (12678/18816)\n",
            "147 391 Loss: 0.922 | Acc: 67.383% (12765/18944)\n",
            "148 391 Loss: 0.922 | Acc: 67.418% (12858/19072)\n",
            "149 391 Loss: 0.921 | Acc: 67.448% (12950/19200)\n",
            "150 391 Loss: 0.920 | Acc: 67.462% (13039/19328)\n",
            "151 391 Loss: 0.921 | Acc: 67.470% (13127/19456)\n",
            "152 391 Loss: 0.920 | Acc: 67.514% (13222/19584)\n",
            "153 391 Loss: 0.918 | Acc: 67.538% (13313/19712)\n",
            "154 391 Loss: 0.918 | Acc: 67.576% (13407/19840)\n",
            "155 391 Loss: 0.916 | Acc: 67.638% (13506/19968)\n",
            "156 391 Loss: 0.916 | Acc: 67.680% (13601/20096)\n",
            "157 391 Loss: 0.914 | Acc: 67.766% (13705/20224)\n",
            "158 391 Loss: 0.914 | Acc: 67.757% (13790/20352)\n",
            "159 391 Loss: 0.913 | Acc: 67.778% (13881/20480)\n",
            "160 391 Loss: 0.913 | Acc: 67.794% (13971/20608)\n",
            "161 391 Loss: 0.913 | Acc: 67.766% (14052/20736)\n",
            "162 391 Loss: 0.912 | Acc: 67.820% (14150/20864)\n",
            "163 391 Loss: 0.911 | Acc: 67.869% (14247/20992)\n",
            "164 391 Loss: 0.909 | Acc: 67.893% (14339/21120)\n",
            "165 391 Loss: 0.909 | Acc: 67.917% (14431/21248)\n",
            "166 391 Loss: 0.908 | Acc: 67.987% (14533/21376)\n",
            "167 391 Loss: 0.909 | Acc: 67.983% (14619/21504)\n",
            "168 391 Loss: 0.909 | Acc: 68.006% (14711/21632)\n",
            "169 391 Loss: 0.909 | Acc: 68.024% (14802/21760)\n",
            "170 391 Loss: 0.909 | Acc: 68.033% (14891/21888)\n",
            "171 391 Loss: 0.908 | Acc: 68.064% (14985/22016)\n",
            "172 391 Loss: 0.909 | Acc: 68.036% (15066/22144)\n",
            "173 391 Loss: 0.910 | Acc: 68.027% (15151/22272)\n",
            "174 391 Loss: 0.910 | Acc: 68.004% (15233/22400)\n",
            "175 391 Loss: 0.909 | Acc: 68.004% (15320/22528)\n",
            "176 391 Loss: 0.909 | Acc: 68.026% (15412/22656)\n",
            "177 391 Loss: 0.908 | Acc: 68.021% (15498/22784)\n",
            "178 391 Loss: 0.907 | Acc: 68.060% (15594/22912)\n",
            "179 391 Loss: 0.906 | Acc: 68.095% (15689/23040)\n",
            "180 391 Loss: 0.905 | Acc: 68.103% (15778/23168)\n",
            "181 391 Loss: 0.906 | Acc: 68.080% (15860/23296)\n",
            "182 391 Loss: 0.904 | Acc: 68.135% (15960/23424)\n",
            "183 391 Loss: 0.903 | Acc: 68.190% (16060/23552)\n",
            "184 391 Loss: 0.903 | Acc: 68.226% (16156/23680)\n",
            "185 391 Loss: 0.902 | Acc: 68.263% (16252/23808)\n",
            "186 391 Loss: 0.901 | Acc: 68.270% (16341/23936)\n",
            "187 391 Loss: 0.901 | Acc: 68.280% (16431/24064)\n",
            "188 391 Loss: 0.900 | Acc: 68.312% (16526/24192)\n",
            "189 391 Loss: 0.900 | Acc: 68.302% (16611/24320)\n",
            "190 391 Loss: 0.900 | Acc: 68.284% (16694/24448)\n",
            "191 391 Loss: 0.899 | Acc: 68.323% (16791/24576)\n",
            "192 391 Loss: 0.898 | Acc: 68.321% (16878/24704)\n",
            "193 391 Loss: 0.898 | Acc: 68.315% (16964/24832)\n",
            "194 391 Loss: 0.897 | Acc: 68.357% (17062/24960)\n",
            "195 391 Loss: 0.896 | Acc: 68.399% (17160/25088)\n",
            "196 391 Loss: 0.896 | Acc: 68.429% (17255/25216)\n",
            "197 391 Loss: 0.896 | Acc: 68.454% (17349/25344)\n",
            "198 391 Loss: 0.895 | Acc: 68.475% (17442/25472)\n",
            "199 391 Loss: 0.895 | Acc: 68.496% (17535/25600)\n",
            "200 391 Loss: 0.894 | Acc: 68.513% (17627/25728)\n",
            "201 391 Loss: 0.894 | Acc: 68.510% (17714/25856)\n",
            "202 391 Loss: 0.894 | Acc: 68.496% (17798/25984)\n",
            "203 391 Loss: 0.893 | Acc: 68.536% (17896/26112)\n",
            "204 391 Loss: 0.893 | Acc: 68.537% (17984/26240)\n",
            "205 391 Loss: 0.893 | Acc: 68.557% (18077/26368)\n",
            "206 391 Loss: 0.893 | Acc: 68.576% (18170/26496)\n",
            "207 391 Loss: 0.892 | Acc: 68.630% (18272/26624)\n",
            "208 391 Loss: 0.892 | Acc: 68.645% (18364/26752)\n",
            "209 391 Loss: 0.891 | Acc: 68.661% (18456/26880)\n",
            "210 391 Loss: 0.891 | Acc: 68.676% (18548/27008)\n",
            "211 391 Loss: 0.890 | Acc: 68.695% (18641/27136)\n",
            "212 391 Loss: 0.890 | Acc: 68.717% (18735/27264)\n",
            "213 391 Loss: 0.889 | Acc: 68.739% (18829/27392)\n",
            "214 391 Loss: 0.888 | Acc: 68.757% (18922/27520)\n",
            "215 391 Loss: 0.888 | Acc: 68.783% (19017/27648)\n",
            "216 391 Loss: 0.887 | Acc: 68.790% (19107/27776)\n",
            "217 391 Loss: 0.887 | Acc: 68.811% (19201/27904)\n",
            "218 391 Loss: 0.887 | Acc: 68.804% (19287/28032)\n",
            "219 391 Loss: 0.887 | Acc: 68.800% (19374/28160)\n",
            "220 391 Loss: 0.887 | Acc: 68.828% (19470/28288)\n",
            "221 391 Loss: 0.887 | Acc: 68.813% (19554/28416)\n",
            "222 391 Loss: 0.886 | Acc: 68.852% (19653/28544)\n",
            "223 391 Loss: 0.886 | Acc: 68.890% (19752/28672)\n",
            "224 391 Loss: 0.885 | Acc: 68.906% (19845/28800)\n",
            "225 391 Loss: 0.884 | Acc: 68.930% (19940/28928)\n",
            "226 391 Loss: 0.884 | Acc: 68.963% (20038/29056)\n",
            "227 391 Loss: 0.883 | Acc: 68.980% (20131/29184)\n",
            "228 391 Loss: 0.882 | Acc: 68.992% (20223/29312)\n",
            "229 391 Loss: 0.883 | Acc: 69.015% (20318/29440)\n",
            "230 391 Loss: 0.882 | Acc: 69.061% (20420/29568)\n",
            "231 391 Loss: 0.881 | Acc: 69.053% (20506/29696)\n",
            "232 391 Loss: 0.881 | Acc: 69.052% (20594/29824)\n",
            "233 391 Loss: 0.881 | Acc: 69.064% (20686/29952)\n",
            "234 391 Loss: 0.881 | Acc: 69.049% (20770/30080)\n",
            "235 391 Loss: 0.880 | Acc: 69.061% (20862/30208)\n",
            "236 391 Loss: 0.879 | Acc: 69.106% (20964/30336)\n",
            "237 391 Loss: 0.879 | Acc: 69.114% (21055/30464)\n",
            "238 391 Loss: 0.879 | Acc: 69.139% (21151/30592)\n",
            "239 391 Loss: 0.879 | Acc: 69.144% (21241/30720)\n",
            "240 391 Loss: 0.878 | Acc: 69.175% (21339/30848)\n",
            "241 391 Loss: 0.878 | Acc: 69.176% (21428/30976)\n",
            "242 391 Loss: 0.877 | Acc: 69.194% (21522/31104)\n",
            "243 391 Loss: 0.878 | Acc: 69.169% (21603/31232)\n",
            "244 391 Loss: 0.877 | Acc: 69.190% (21698/31360)\n",
            "245 391 Loss: 0.877 | Acc: 69.195% (21788/31488)\n",
            "246 391 Loss: 0.877 | Acc: 69.202% (21879/31616)\n",
            "247 391 Loss: 0.876 | Acc: 69.219% (21973/31744)\n",
            "248 391 Loss: 0.876 | Acc: 69.236% (22067/31872)\n",
            "249 391 Loss: 0.876 | Acc: 69.263% (22164/32000)\n",
            "250 391 Loss: 0.875 | Acc: 69.267% (22254/32128)\n",
            "251 391 Loss: 0.874 | Acc: 69.299% (22353/32256)\n",
            "252 391 Loss: 0.874 | Acc: 69.318% (22448/32384)\n",
            "253 391 Loss: 0.873 | Acc: 69.341% (22544/32512)\n",
            "254 391 Loss: 0.872 | Acc: 69.350% (22636/32640)\n",
            "255 391 Loss: 0.873 | Acc: 69.351% (22725/32768)\n",
            "256 391 Loss: 0.872 | Acc: 69.382% (22824/32896)\n",
            "257 391 Loss: 0.872 | Acc: 69.398% (22918/33024)\n",
            "258 391 Loss: 0.870 | Acc: 69.444% (23022/33152)\n",
            "259 391 Loss: 0.870 | Acc: 69.453% (23114/33280)\n",
            "260 391 Loss: 0.869 | Acc: 69.483% (23213/33408)\n",
            "261 391 Loss: 0.869 | Acc: 69.481% (23301/33536)\n",
            "262 391 Loss: 0.869 | Acc: 69.499% (23396/33664)\n",
            "263 391 Loss: 0.869 | Acc: 69.513% (23490/33792)\n",
            "264 391 Loss: 0.868 | Acc: 69.522% (23582/33920)\n",
            "265 391 Loss: 0.868 | Acc: 69.546% (23679/34048)\n",
            "266 391 Loss: 0.867 | Acc: 69.593% (23784/34176)\n",
            "267 391 Loss: 0.866 | Acc: 69.613% (23880/34304)\n",
            "268 391 Loss: 0.866 | Acc: 69.624% (23973/34432)\n",
            "269 391 Loss: 0.865 | Acc: 69.644% (24069/34560)\n",
            "270 391 Loss: 0.864 | Acc: 69.684% (24172/34688)\n",
            "271 391 Loss: 0.863 | Acc: 69.712% (24271/34816)\n",
            "272 391 Loss: 0.863 | Acc: 69.726% (24365/34944)\n",
            "273 391 Loss: 0.863 | Acc: 69.739% (24459/35072)\n",
            "274 391 Loss: 0.862 | Acc: 69.753% (24553/35200)\n",
            "275 391 Loss: 0.861 | Acc: 69.769% (24648/35328)\n",
            "276 391 Loss: 0.861 | Acc: 69.782% (24742/35456)\n",
            "277 391 Loss: 0.860 | Acc: 69.807% (24840/35584)\n",
            "278 391 Loss: 0.860 | Acc: 69.814% (24932/35712)\n",
            "279 391 Loss: 0.859 | Acc: 69.841% (25031/35840)\n",
            "280 391 Loss: 0.859 | Acc: 69.865% (25129/35968)\n",
            "281 391 Loss: 0.858 | Acc: 69.869% (25220/36096)\n",
            "282 391 Loss: 0.857 | Acc: 69.896% (25319/36224)\n",
            "283 391 Loss: 0.857 | Acc: 69.916% (25416/36352)\n",
            "284 391 Loss: 0.856 | Acc: 69.926% (25509/36480)\n",
            "285 391 Loss: 0.856 | Acc: 69.960% (25611/36608)\n",
            "286 391 Loss: 0.855 | Acc: 69.967% (25703/36736)\n",
            "287 391 Loss: 0.855 | Acc: 69.992% (25802/36864)\n",
            "288 391 Loss: 0.854 | Acc: 70.029% (25905/36992)\n",
            "289 391 Loss: 0.854 | Acc: 70.048% (26002/37120)\n",
            "290 391 Loss: 0.854 | Acc: 70.044% (26090/37248)\n",
            "291 391 Loss: 0.854 | Acc: 70.026% (26173/37376)\n",
            "292 391 Loss: 0.853 | Acc: 70.043% (26269/37504)\n",
            "293 391 Loss: 0.853 | Acc: 70.057% (26364/37632)\n",
            "294 391 Loss: 0.852 | Acc: 70.093% (26467/37760)\n",
            "295 391 Loss: 0.852 | Acc: 70.083% (26553/37888)\n",
            "296 391 Loss: 0.852 | Acc: 70.089% (26645/38016)\n",
            "297 391 Loss: 0.852 | Acc: 70.103% (26740/38144)\n",
            "298 391 Loss: 0.852 | Acc: 70.103% (26830/38272)\n",
            "299 391 Loss: 0.852 | Acc: 70.112% (26923/38400)\n",
            "300 391 Loss: 0.851 | Acc: 70.126% (27018/38528)\n",
            "301 391 Loss: 0.852 | Acc: 70.108% (27101/38656)\n",
            "302 391 Loss: 0.852 | Acc: 70.104% (27189/38784)\n",
            "303 391 Loss: 0.851 | Acc: 70.122% (27286/38912)\n",
            "304 391 Loss: 0.851 | Acc: 70.131% (27379/39040)\n",
            "305 391 Loss: 0.851 | Acc: 70.126% (27467/39168)\n",
            "306 391 Loss: 0.851 | Acc: 70.134% (27560/39296)\n",
            "307 391 Loss: 0.851 | Acc: 70.140% (27652/39424)\n",
            "308 391 Loss: 0.850 | Acc: 70.171% (27754/39552)\n",
            "309 391 Loss: 0.850 | Acc: 70.197% (27854/39680)\n",
            "310 391 Loss: 0.849 | Acc: 70.212% (27950/39808)\n",
            "311 391 Loss: 0.849 | Acc: 70.225% (28045/39936)\n",
            "312 391 Loss: 0.848 | Acc: 70.253% (28146/40064)\n",
            "313 391 Loss: 0.849 | Acc: 70.220% (28223/40192)\n",
            "314 391 Loss: 0.849 | Acc: 70.221% (28313/40320)\n",
            "315 391 Loss: 0.848 | Acc: 70.241% (28411/40448)\n",
            "316 391 Loss: 0.848 | Acc: 70.236% (28499/40576)\n",
            "317 391 Loss: 0.847 | Acc: 70.258% (28598/40704)\n",
            "318 391 Loss: 0.847 | Acc: 70.256% (28687/40832)\n",
            "319 391 Loss: 0.847 | Acc: 70.249% (28774/40960)\n",
            "320 391 Loss: 0.847 | Acc: 70.276% (28875/41088)\n",
            "321 391 Loss: 0.846 | Acc: 70.283% (28968/41216)\n",
            "322 391 Loss: 0.846 | Acc: 70.276% (29055/41344)\n",
            "323 391 Loss: 0.846 | Acc: 70.291% (29151/41472)\n",
            "324 391 Loss: 0.845 | Acc: 70.310% (29249/41600)\n",
            "325 391 Loss: 0.844 | Acc: 70.327% (29346/41728)\n",
            "326 391 Loss: 0.844 | Acc: 70.327% (29436/41856)\n",
            "327 391 Loss: 0.844 | Acc: 70.358% (29539/41984)\n",
            "328 391 Loss: 0.844 | Acc: 70.365% (29632/42112)\n",
            "329 391 Loss: 0.844 | Acc: 70.376% (29727/42240)\n",
            "330 391 Loss: 0.844 | Acc: 70.395% (29825/42368)\n",
            "331 391 Loss: 0.843 | Acc: 70.425% (29928/42496)\n",
            "332 391 Loss: 0.843 | Acc: 70.430% (30020/42624)\n",
            "333 391 Loss: 0.843 | Acc: 70.453% (30120/42752)\n",
            "334 391 Loss: 0.842 | Acc: 70.473% (30219/42880)\n",
            "335 391 Loss: 0.842 | Acc: 70.466% (30306/43008)\n",
            "336 391 Loss: 0.842 | Acc: 70.479% (30402/43136)\n",
            "337 391 Loss: 0.841 | Acc: 70.504% (30503/43264)\n",
            "338 391 Loss: 0.841 | Acc: 70.497% (30590/43392)\n",
            "339 391 Loss: 0.840 | Acc: 70.503% (30683/43520)\n",
            "340 391 Loss: 0.840 | Acc: 70.519% (30780/43648)\n",
            "341 391 Loss: 0.840 | Acc: 70.525% (30873/43776)\n",
            "342 391 Loss: 0.839 | Acc: 70.533% (30967/43904)\n",
            "343 391 Loss: 0.839 | Acc: 70.535% (31058/44032)\n",
            "344 391 Loss: 0.839 | Acc: 70.555% (31157/44160)\n",
            "345 391 Loss: 0.839 | Acc: 70.577% (31257/44288)\n",
            "346 391 Loss: 0.839 | Acc: 70.587% (31352/44416)\n",
            "347 391 Loss: 0.838 | Acc: 70.613% (31454/44544)\n",
            "348 391 Loss: 0.838 | Acc: 70.604% (31540/44672)\n",
            "349 391 Loss: 0.838 | Acc: 70.605% (31631/44800)\n",
            "350 391 Loss: 0.838 | Acc: 70.624% (31730/44928)\n",
            "351 391 Loss: 0.837 | Acc: 70.643% (31829/45056)\n",
            "352 391 Loss: 0.837 | Acc: 70.647% (31921/45184)\n",
            "353 391 Loss: 0.837 | Acc: 70.655% (32015/45312)\n",
            "354 391 Loss: 0.837 | Acc: 70.660% (32108/45440)\n",
            "355 391 Loss: 0.837 | Acc: 70.661% (32199/45568)\n",
            "356 391 Loss: 0.836 | Acc: 70.660% (32289/45696)\n",
            "357 391 Loss: 0.836 | Acc: 70.666% (32382/45824)\n",
            "358 391 Loss: 0.836 | Acc: 70.663% (32471/45952)\n",
            "359 391 Loss: 0.836 | Acc: 70.677% (32568/46080)\n",
            "360 391 Loss: 0.835 | Acc: 70.698% (32668/46208)\n",
            "361 391 Loss: 0.834 | Acc: 70.714% (32766/46336)\n",
            "362 391 Loss: 0.834 | Acc: 70.721% (32860/46464)\n",
            "363 391 Loss: 0.833 | Acc: 70.744% (32961/46592)\n",
            "364 391 Loss: 0.833 | Acc: 70.743% (33051/46720)\n",
            "365 391 Loss: 0.832 | Acc: 70.761% (33150/46848)\n",
            "366 391 Loss: 0.832 | Acc: 70.766% (33243/46976)\n",
            "367 391 Loss: 0.832 | Acc: 70.786% (33343/47104)\n",
            "368 391 Loss: 0.831 | Acc: 70.791% (33436/47232)\n",
            "369 391 Loss: 0.831 | Acc: 70.798% (33530/47360)\n",
            "370 391 Loss: 0.831 | Acc: 70.818% (33630/47488)\n",
            "371 391 Loss: 0.830 | Acc: 70.840% (33731/47616)\n",
            "372 391 Loss: 0.830 | Acc: 70.863% (33833/47744)\n",
            "373 391 Loss: 0.829 | Acc: 70.874% (33929/47872)\n",
            "374 391 Loss: 0.829 | Acc: 70.898% (34031/48000)\n",
            "375 391 Loss: 0.829 | Acc: 70.907% (34126/48128)\n",
            "376 391 Loss: 0.828 | Acc: 70.926% (34226/48256)\n",
            "377 391 Loss: 0.827 | Acc: 70.961% (34334/48384)\n",
            "378 391 Loss: 0.827 | Acc: 70.964% (34426/48512)\n",
            "379 391 Loss: 0.826 | Acc: 70.989% (34529/48640)\n",
            "380 391 Loss: 0.826 | Acc: 71.008% (34629/48768)\n",
            "381 391 Loss: 0.825 | Acc: 71.024% (34728/48896)\n",
            "382 391 Loss: 0.825 | Acc: 71.031% (34822/49024)\n",
            "383 391 Loss: 0.825 | Acc: 71.051% (34923/49152)\n",
            "384 391 Loss: 0.825 | Acc: 71.053% (35015/49280)\n",
            "385 391 Loss: 0.825 | Acc: 71.069% (35114/49408)\n",
            "386 391 Loss: 0.824 | Acc: 71.082% (35211/49536)\n",
            "387 391 Loss: 0.825 | Acc: 71.084% (35303/49664)\n",
            "388 391 Loss: 0.824 | Acc: 71.104% (35404/49792)\n",
            "389 391 Loss: 0.823 | Acc: 71.124% (35505/49920)\n",
            "390 391 Loss: 0.823 | Acc: 71.134% (35567/50000)\n",
            "0 100 Loss: 1.005 | Acc: 72.000% (72/100)\n",
            "1 100 Loss: 0.919 | Acc: 73.500% (147/200)\n",
            "2 100 Loss: 0.872 | Acc: 72.667% (218/300)\n",
            "3 100 Loss: 0.852 | Acc: 72.000% (288/400)\n",
            "4 100 Loss: 0.874 | Acc: 71.000% (355/500)\n",
            "5 100 Loss: 0.860 | Acc: 72.000% (432/600)\n",
            "6 100 Loss: 0.868 | Acc: 71.571% (501/700)\n",
            "7 100 Loss: 0.923 | Acc: 70.125% (561/800)\n",
            "8 100 Loss: 0.947 | Acc: 69.444% (625/900)\n",
            "9 100 Loss: 0.940 | Acc: 70.000% (700/1000)\n",
            "10 100 Loss: 0.928 | Acc: 70.364% (774/1100)\n",
            "11 100 Loss: 0.939 | Acc: 70.417% (845/1200)\n",
            "12 100 Loss: 0.928 | Acc: 70.692% (919/1300)\n",
            "13 100 Loss: 0.937 | Acc: 70.500% (987/1400)\n",
            "14 100 Loss: 0.932 | Acc: 70.733% (1061/1500)\n",
            "15 100 Loss: 0.939 | Acc: 70.500% (1128/1600)\n",
            "16 100 Loss: 0.939 | Acc: 70.588% (1200/1700)\n",
            "17 100 Loss: 0.934 | Acc: 70.278% (1265/1800)\n",
            "18 100 Loss: 0.936 | Acc: 70.368% (1337/1900)\n",
            "19 100 Loss: 0.939 | Acc: 70.200% (1404/2000)\n",
            "20 100 Loss: 0.937 | Acc: 70.143% (1473/2100)\n",
            "21 100 Loss: 0.947 | Acc: 70.091% (1542/2200)\n",
            "22 100 Loss: 0.946 | Acc: 69.957% (1609/2300)\n",
            "23 100 Loss: 0.944 | Acc: 70.000% (1680/2400)\n",
            "24 100 Loss: 0.938 | Acc: 70.160% (1754/2500)\n",
            "25 100 Loss: 0.951 | Acc: 70.115% (1823/2600)\n",
            "26 100 Loss: 0.944 | Acc: 70.296% (1898/2700)\n",
            "27 100 Loss: 0.941 | Acc: 70.321% (1969/2800)\n",
            "28 100 Loss: 0.943 | Acc: 70.414% (2042/2900)\n",
            "29 100 Loss: 0.939 | Acc: 70.600% (2118/3000)\n",
            "30 100 Loss: 0.943 | Acc: 70.645% (2190/3100)\n",
            "31 100 Loss: 0.940 | Acc: 70.594% (2259/3200)\n",
            "32 100 Loss: 0.942 | Acc: 70.576% (2329/3300)\n",
            "33 100 Loss: 0.949 | Acc: 70.206% (2387/3400)\n",
            "34 100 Loss: 0.948 | Acc: 70.343% (2462/3500)\n",
            "35 100 Loss: 0.949 | Acc: 70.333% (2532/3600)\n",
            "36 100 Loss: 0.951 | Acc: 70.324% (2602/3700)\n",
            "37 100 Loss: 0.953 | Acc: 70.237% (2669/3800)\n",
            "38 100 Loss: 0.950 | Acc: 70.231% (2739/3900)\n",
            "39 100 Loss: 0.951 | Acc: 70.175% (2807/4000)\n",
            "40 100 Loss: 0.952 | Acc: 70.268% (2881/4100)\n",
            "41 100 Loss: 0.957 | Acc: 70.190% (2948/4200)\n",
            "42 100 Loss: 0.955 | Acc: 70.256% (3021/4300)\n",
            "43 100 Loss: 0.949 | Acc: 70.273% (3092/4400)\n",
            "44 100 Loss: 0.944 | Acc: 70.378% (3167/4500)\n",
            "45 100 Loss: 0.942 | Acc: 70.326% (3235/4600)\n",
            "46 100 Loss: 0.935 | Acc: 70.426% (3310/4700)\n",
            "47 100 Loss: 0.941 | Acc: 70.354% (3377/4800)\n",
            "48 100 Loss: 0.942 | Acc: 70.224% (3441/4900)\n",
            "49 100 Loss: 0.944 | Acc: 70.260% (3513/5000)\n",
            "50 100 Loss: 0.939 | Acc: 70.412% (3591/5100)\n",
            "51 100 Loss: 0.940 | Acc: 70.385% (3660/5200)\n",
            "52 100 Loss: 0.942 | Acc: 70.283% (3725/5300)\n",
            "53 100 Loss: 0.942 | Acc: 70.278% (3795/5400)\n",
            "54 100 Loss: 0.941 | Acc: 70.291% (3866/5500)\n",
            "55 100 Loss: 0.941 | Acc: 70.268% (3935/5600)\n",
            "56 100 Loss: 0.943 | Acc: 70.228% (4003/5700)\n",
            "57 100 Loss: 0.940 | Acc: 70.224% (4073/5800)\n",
            "58 100 Loss: 0.944 | Acc: 70.085% (4135/5900)\n",
            "59 100 Loss: 0.949 | Acc: 69.950% (4197/6000)\n",
            "60 100 Loss: 0.946 | Acc: 69.984% (4269/6100)\n",
            "61 100 Loss: 0.945 | Acc: 69.903% (4334/6200)\n",
            "62 100 Loss: 0.945 | Acc: 69.937% (4406/6300)\n",
            "63 100 Loss: 0.945 | Acc: 70.000% (4480/6400)\n",
            "64 100 Loss: 0.946 | Acc: 69.938% (4546/6500)\n",
            "65 100 Loss: 0.946 | Acc: 69.909% (4614/6600)\n",
            "66 100 Loss: 0.945 | Acc: 69.836% (4679/6700)\n",
            "67 100 Loss: 0.945 | Acc: 69.838% (4749/6800)\n",
            "68 100 Loss: 0.945 | Acc: 69.841% (4819/6900)\n",
            "69 100 Loss: 0.945 | Acc: 69.914% (4894/7000)\n",
            "70 100 Loss: 0.946 | Acc: 69.915% (4964/7100)\n",
            "71 100 Loss: 0.943 | Acc: 70.000% (5040/7200)\n",
            "72 100 Loss: 0.945 | Acc: 69.863% (5100/7300)\n",
            "73 100 Loss: 0.942 | Acc: 69.878% (5171/7400)\n",
            "74 100 Loss: 0.940 | Acc: 69.933% (5245/7500)\n",
            "75 100 Loss: 0.939 | Acc: 69.961% (5317/7600)\n",
            "76 100 Loss: 0.939 | Acc: 69.961% (5387/7700)\n",
            "77 100 Loss: 0.939 | Acc: 69.949% (5456/7800)\n",
            "78 100 Loss: 0.939 | Acc: 69.962% (5527/7900)\n",
            "79 100 Loss: 0.938 | Acc: 69.975% (5598/8000)\n",
            "80 100 Loss: 0.937 | Acc: 70.012% (5671/8100)\n",
            "81 100 Loss: 0.937 | Acc: 69.988% (5739/8200)\n",
            "82 100 Loss: 0.936 | Acc: 69.988% (5809/8300)\n",
            "83 100 Loss: 0.937 | Acc: 69.917% (5873/8400)\n",
            "84 100 Loss: 0.939 | Acc: 69.812% (5934/8500)\n",
            "85 100 Loss: 0.940 | Acc: 69.802% (6003/8600)\n",
            "86 100 Loss: 0.942 | Acc: 69.747% (6068/8700)\n",
            "87 100 Loss: 0.944 | Acc: 69.761% (6139/8800)\n",
            "88 100 Loss: 0.944 | Acc: 69.809% (6213/8900)\n",
            "89 100 Loss: 0.944 | Acc: 69.800% (6282/9000)\n",
            "90 100 Loss: 0.942 | Acc: 69.835% (6355/9100)\n",
            "91 100 Loss: 0.938 | Acc: 69.957% (6436/9200)\n",
            "92 100 Loss: 0.937 | Acc: 69.935% (6504/9300)\n",
            "93 100 Loss: 0.939 | Acc: 69.904% (6571/9400)\n",
            "94 100 Loss: 0.936 | Acc: 69.947% (6645/9500)\n",
            "95 100 Loss: 0.935 | Acc: 69.969% (6717/9600)\n",
            "96 100 Loss: 0.933 | Acc: 70.000% (6790/9700)\n",
            "97 100 Loss: 0.936 | Acc: 69.939% (6854/9800)\n",
            "98 100 Loss: 0.936 | Acc: 69.929% (6923/9900)\n",
            "99 100 Loss: 0.936 | Acc: 69.900% (6990/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}